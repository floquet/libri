\clearpage

\begin{xcb}{Exercises}
\begin{enumerate}
\item Follow the same procedure here to compute the following singular value decompositions:
$$
\begin{array}{ ccccccrr }
   &
\left[
\begin{array}{ rr }
 1 & 0 \\
 0 & 0
\end{array}
\right]
   & = & \svd{T} & = 
   & \itwo
   &
\left[
\begin{array}{ cc }
 1 & 0 \\
 0 & 0
\end{array}
\right]
   & \itwo \\[12pt]
%%%%%%%%%%%%%%%%%%%%
   &
\left[
\begin{array}{ rr }
 1 & 1 \\
 0 & 0
\end{array}
\right]
   & = & \svd{T} & = 
   & \itwo
   &
\left[
\begin{array}{ cc }
 \sqrt{2} & 0 \\
 0 & 0
\end{array}
\right]
   & \frac{ 1 }{ \sqrt{2} }
   \left[
   \begin{array}{ rr }
     1 & 1 \\
     -1 & 1
   \end{array}
   \right]
   \\[12pt]
%%%%%%%%%%%%%%%%%%%%
   &
\left[
\begin{array}{ rr }
 1 & 1 \\
 1 & 1
\end{array}
\right]
   & = & \svd{T} & = 
   & \itwo
   &
\left[
\begin{array}{ cc }
 2 & 0 \\
 0 & 0
\end{array}
\right]
   & \frac{ 1 }{ \sqrt{2} }
   \left[
   \begin{array}{ rr }
     1 & 1 \\
     -1 & 1
   \end{array}
   \right]
   \\[12pt]
%%%%%%%%%%%%%%%%%%%%
   &
\left[
\begin{array}{ rr }
 1 & i \\
 0 & 0
\end{array}
\right]
   & = & \svd{*} & = 
   & \itwo
   &
\left[
\begin{array}{ cc }
 \sqrt{2} & 0 \\
 0 & 0
\end{array}
\right]
   & \frac{ 1 }{ \sqrt{2} }
   \left[
   \begin{array}{ cc }
     1 & i \\
     -i & 1
   \end{array}
   \right]
   \\[12pt]
\end{array}
$$
What is the relationship between the singular values and the Frobenius norm of the target matrix? Recall that for $ \A{} \in \cmplx{ m \times m } $ the Frobenius norm is
\begin{equation}
  \norm{ \A{} }_F = \left[ \sum_{ r = 1 }^{ m }{ \sum_{ c = 1 }^{ m }{ \abs{ \A{}_{\paren{ r,c }} }^2 } } \right]^{ \frac{ 1 }{ 2 } }.
\end{equation}
\item We have seen that at least some matrices decompose easily; now we will see that at least one matrix is nightmarish. Feel free to skip the algebra in part (a) and move on to the conceptual questions that follow.
\begin{description}
\item[a)] 
Consider the innocuous matrix
\begin{equation}
  \svd{T} =
\left[
\begin{array}{ rr }
 1 & 2 \\
 0 & 2
\end{array}
\right].
\end{equation}
Given that
\begin{equation}
  \begin{split}
\textbf{Y} & = 
\left[
\begin{array}{ cc }
 \sqrt{\frac{1}{130} \left[65+\sqrt{65}\right]} &
   -\sqrt{\frac{1}{130} \left[65-\sqrt{65}\right]} \\[5pt]
 4 \sqrt{\frac{2}{65+\sqrt{65}}} & \sqrt{\frac{1}{130}
   \left[65+\sqrt{65}\right]}
\end{array}
\right], \\ 
\X{} & =
\left[
\begin{array}{ cc }
 \sqrt{\frac{1}{2}-\frac{7}{2 \sqrt{65}}} &
   -\sqrt{\frac{1}{2}+\frac{7}{2 \sqrt{65}}} \\[5pt]
 \sqrt{\frac{1}{2}+\frac{7}{2 \sqrt{65}}} &
   \sqrt{\frac{1}{2}-\frac{7}{2 \sqrt{65}}}
\end{array}
\right]
  \end{split}
\end{equation}
show that
\begin{equation}
  \Sigma =
\left[
\begin{array}{ cc }
 \sqrt{\frac{1}{2} \left[9+\sqrt{65}\right]} & 0 \\
 0 & \sqrt{\frac{1}{2} \left[9-\sqrt{65}\right]}
\end{array}
\right].
\end{equation}
  \item[b)] It's all a question of perspective really. You can write the coordinate matrices $ \textbf{Y} $ and $ \X{} $ as  rotation matrices:
\begin{equation}
\begin{split}
 \textbf{Y} & = 
\left[ 
\begin{array}{ rr }
  \cos \theta_y & - \sin \theta_y \\
  \sin \theta_y & \cos \theta_y
\end{array}
\right] = \textbf{R}(\theta_y), \\
 \X{} & = 
\left[ 
\begin{array}{ rr }
  \cos \theta_x & - \sin \theta_x \\
  \sin \theta_x & \cos \theta_x
\end{array}
\right] = \textbf{R}(\theta_x).
\end{split}
\end{equation}
Find the small parameters $ \epsilon_x $ and $ \epsilon_y $ such that
\begin{equation}
  \begin{split}
    \theta_x & = \frac{ 5 \pi }{ 12 } \left[ 1 + \epsilon_x \right], \\
    \theta_y & = \frac{ 2 \pi }{ 9 } \left[ 1 + \epsilon_y \right]. \\
  \end{split}
\end{equation}
We see that the algebraic complexity of these solutions comes from finding closed forms for these rotation angles. 
  \item[c)] This alludes to a class of $ 2 \times 2 $ matrices with decompositions of the form
\begin{equation}
  \textbf{A}^{ 2 \times 2 } = e^{ i \theta_y } \ \Sigma \  e^{ -i \theta_x }.
\end{equation}
Why does the exponential on the right-hand side have a negative sign?
  \item[d)] Below is a plot showing spans of the domain. The first set of longer vectors is from using the rows vectors as a span. Is the angle between these two vectors acute, right or oblique? The second set are unit vectors from the $ columns $ of $ \textbf{Y} $. Is the angle between these two vectors acute, right or oblique? Where is the rotation angle $ \theta_x $? Which of the matrices in exercise 1 are in this class?
  \item[e)] Prepare a similar plot for the image. Plot the column vectors of $ \textbf{A} $ against the $ columns $ of $ \textbf{Y} $.
\end{description}
%%%
\item Show that all the least squares solutions to
\begin{equation}
  \begin{split}
    \lsa \\
 \left[
\begin{array}{ rr }
 1 & -1 \\
 -1 & 1 \\
\end{array}
\right]
\left[
\begin{array}{l}
 x_1 \\
 x_2
\end{array}
\right]
  & =
\left[
\begin{array}{l}
 2 \\
 1 \\
\end{array}
\right].
  \end{split}
\end{equation}
are given by 
\begin{equation}
  x = \frac{ 1 }{ 4 }
\left[
\begin{array}{ r }
 1 \\
 -1
\end{array}
\right]
  + \alpha
\left[
\begin{array}{l}
 1 \\
 1
\end{array}
\right]
\end{equation}
where $ \alpha $ can be any complex number.
%%
\item The Pauli spin matrices are a set of $ 2 \times 2 $ Hermitian, unitary and involutary matrices. They are defined as
\begin{equation}
  \begin{split}
    \sigma_1 = \paulia, \qquad \sigma_2 = \paulib, \qquad \sigma_3 = \paulic. \\
  \end{split}
\end{equation}
\begin{description}
  \item[a)] For practice, verify each of the properties mentioned for all three matrices. That is, show that they are Hermitian
\begin{equation}
  \sigma_k = \overline{ \sigma_{k}^{T} } = \sigma_k^{ * };
\end{equation}
they are unitary
\begin{equation}
  \sigma_{k}^{*} = \sigma_{k}^{ -1 };
\end{equation}
and they are involutary
\begin{equation}
  \sigma_k^2 = \I{2}.
\end{equation}
  \item[b)] Verify that the matrices are traceless and that the determinant is $ -1 $. What does this tell us about the eigenvalues?
  \item[c)] For each of the matrices
\begin{equation}
  \Sigma_k = \I{2} = \itwo, \qquad
  \X{}_k = \itwo.
\end{equation}
Compute all three $ \textbf{Y} $ matrices to verify these singular value decompositions:
%%
\begin{equation}
  \begin{array}{ccccrcc}
    \sigma_{1} & = & \svdtag{1} & = & \I{2} & \I{2} & \sigma_{2}, \\[3pt]
    \sigma_{2} & = & \svdtag{2} & = & -i \sigma_{3} & \I{2} & \sigma_{2}, \\[3pt]
    \sigma_{3} & = & \svdtag{3} & = &  i \sigma_{2} & \I{2} & \sigma_{2}. \\
  \end{array}
\end{equation}
\end{description}
%%%
\item Rotations matrices in the plane take the form
\begin{equation}
  \rot = 
  \left[
  \begin{array}{ rr }
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta
  \end{array}
  \right].
\end{equation}
\begin{description}
  \item[a)] Compute the trace and the determinant and relate this to the two eigenvalues.
  \item[b)] To rotate a vector $ \textbf{ x } $ counterclockwise, use the relation
\begin{equation}
  \textbf{ x }' = \left[ 
         \begin{array}{ c }
           x_1'  \\
           x_2'
         \end{array}
         \right]
       = \rot \textbf{ x }.
\end{equation}
Calculate a functional form for $ x' $ in terms of $ x, y, \theta $.
  \item[c)] In the complex plane multiplication by $ e^{ i \theta } $ rotates a complex number $ z = x + i y $ counterclockwise by an angle $ \theta $, that is $ \Arg( e^{ i \theta } z ) = \Arg( z ) + \theta $. We can see this in simple multiplication in with $ z = r e^{ i \phi } $:
\begin{equation}
   e^{ i \theta } \left[ r e^{ i \phi } \right] = r e^{ i ( \theta + \phi ) }.
\end{equation}
Start with the Taylor expansion for the exponential function
\begin{equation}
\begin{split}
  e^x & = 1 + x + \frac{ 1 }{ 2 } x^2 + \frac{ 1 }{ 6 } x^3 + \frac{ 1 }{ 24 } x^4 + \frac{ 1 }{ 120 } x^5 + \dots + \frac{ 1 }{ n! } x^n + \dots \\
  & = 1 + \sum_{ n = 1 }^{ \infty }{ \frac{ 1 }{ n! } x^n } \\
\end{split}
\end{equation}
and deduce the famous $ Euler $ $ identity $
\begin{equation}
  e^{ i \theta } = \cos \theta + i \sin \theta.
\end{equation}
Now use this formula and the Cartesian form for complex numbers to compute
\begin{equation}
  z' = e^{ i \theta } z
\end{equation}
to show that this is equivalent to the functional form you found in part (b).
\end{description}
%%%
\item Compute a singular value decomposition for these rotation matrices. Using $ \Sigma = \I{2} $ and $ \X{} = \sigma_1 $ show that
\begin{equation}
  \left[
  \begin{array}{ rr }
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta
  \end{array}
  \right]
    =
    \svd{T}
    =
  \left[
  \begin{array}{ rr }
    -\sin \theta & \cos \theta \\
    \cos \theta & \sin \theta
  \end{array}
  \right] \Sigma \X{T}.
\end{equation}
That is, show that $ \textbf{Y} $ is $ \rot $ with the columns exchanged
\begin{equation}
  \textbf{Y} = \ktwo \rot.
  \label{eq:01:last}
\end{equation}
Equation \eqref{eq:01:last} expresses $ \textbf{Y} $ in terms of $ \rot $. Invert this form to express $ \rot $ in terms of $ \textbf{Y} $.
\item After considering the range and codomain of the two formulations, write a single sentence to explain how the equation
\begin{equation}
  \A{}x=b
\end{equation}
may not have a solution but the normal equations
\begin{equation}
  \A{T}\A{}x=\A{T}b
\end{equation}
will have a solution. 
\end{enumerate}

\end{xcb}

\endinput