\section{An economic proof by Laub}

The books by Laub are treasures of economy and clarity. His proof \cite[p. 35]{Laub2005}\index{singular value decomposition!proofs!Laub} of the SVD theorem nurtures the motif of intuition. In many ways, his presentation is a more rigorous presentation of the development presented in chapter \eqref{chap:moreformal}. As such, it is a natural choice to present first.

The proof that follows is verbose so it is presented in a format which separates the central idea of each step from explanatory narrative. To outline, we form a product matrix so that we have an Hermitian (square, symmetric) matrix to work with. We resolve the range space of this matrix. We then resolve the null space. The goal is to find a matrix decomposition which resolves the four fundamental subspaces associated with a matrix.

We start with a matrix $\aicmnr$; a matrix with $m$ rows and $n$ columns of matrix rank $\rho\paren{\le \min\lst{m,n}}$. 
\begin{enumerate}
\item This proof handles the most general case of rectangular matrices. 
\item This matrix may have complex entries. 
\item Of course real matrices are a subset of complex matrices.
\end{enumerate}
%%%
\subsection{Resolve eigensystem for $\wx{*}$} Form the square $\bynn$ matrix $\wx{*}$ which allows us to resolve an \emph{eigensystem} of eigenvalues and eigenvectors. The square matrix $\wx{*}$ is necessary because as we saw in \S\eqref{sec:moreformalbigthree}:
\begin{enumerate}
%
\item the eigenvalues are real,
\item the eigenvalues are nonnegative,
\item the eigenvectors are mutually orthogonal.
\item Aside: We have two choices from the product matrices, $\wx{*}$ and $\wy{*}$; either will work.
%
\end{enumerate}
%%%
\subsubsection{Singular values} Form the square $\bynn$ matrix $\wx{*}$ which allows us to resolve an \emph{eigensystem} of eigenvalues and eigenvectors. The square matrix $\wx{*}$ is necessary because as we saw in \S\eqref{sec:moreformalbigthree}:
\begin{enumerate}
%
\item the eigenvalues are real,
\item the eigenvalues are nonnegative,
\item the eigenvectors are mutually orthogonal.
\item Aside: We have two choices from the product matrices, $\wx{*}$ and $\wy{*}$; either will work.
%
\end{enumerate}
In an act of prescience, label these eigenvalues $\sigma_{k}^{2},\ k=1,n$. Sort the eigenvalues in decreasing order:
\begin{equation}
  \sigma_{1} \ge \sigma_{2} \ge \dots \ge \sigma_{\rho}.
\end{equation}
\subitem The remaining eigenvalues are zero. That is
\begin{equation}
  \lst{ \sigma_{\rho+1}, \sigma_{\rho+2}, \dots, \sigma_{n} } = 0.
\end{equation}
\subitem If the matrix is nonsingular then all of the eigenvalues will be greater than zero.
\subitem The ordering of the eigenvalues in not necessary; it is merely a useful practice.
\subitem Why describe the eigenvalues as squared quantities? When $m=n$ the eigenvalues of $\wx{*}$ are the squares of the eigenvalues of $\A{}$.
\subitem We rely upon the fact that $\text{rank}\paren{\A{}} = \text{rank}\paren{\wx{*}} = \text{rank}\paren{\wy{*}}$.%%%
%
%%%
\subsubsection{Right eigenvectors} Collect and eigenvectors. The square matrix $\wx{*}$ is necessary because as we saw in \S\eqref{sec:moreformalbigthree}:
\begin{enumerate}
%
\item the eigenvalues are real,
\item the eigenvalues are nonnegative,
\item the eigenvectors are mutually orthogonal.
\item Aside: We have two choices from the product matrices, $\wx{*}$ and $\wy{*}$; either will work.
%
\end{enumerate}
\subsubsection{Premultiply by $\V{*}$}
%%%
\subsubsection{Assemble the diagonal matrix $\ess{}$} 
Use the nonzero eigenvalues to assemble the diagonal matrix
\begin{equation}
  \ess{} = \eesd{}.
\end{equation}
%%%
{\bl{\subsubsection{Resolve the range space $\rnga{*}$}}} 
%%%
\subsubsection{Create an identity matrix}
%%%
\subsubsection{Define the rank deficiencies}
%
\begin{enumerate}
\item The column rank deficiency is $\eta_{u}$.
\item The row rank deficiency is $\eta_{v}$.
\end{enumerate}
%
\begin{equation}
  \begin{split}
     \eta_{v} &= n - \rho,\\
     \eta_{u} &= m - \rho.
  \end{split}
\end{equation}
%%%
{\rd{\subsubsection{Resolve the null space $\nlla{}$}}} 
%%%

%%%
\begin{enumerate}
\item Define the rank deficiencies\index{rank deficiency!definitions}
\begin{equation}
  \begin{split}
     \eta_{x} &= n - \rho,\\
     \eta_{y} &= m - \rho.
  \end{split}
\end{equation}
\subitem The column rank deficiency is $\eta_{x}$.
\subitem The row rank deficiency is $\eta_{y}$.
%%%
\item {\rd{ \textbf{Resolve the null space.} }} We have looked at the first $\rho$ singular values. Now we address the final $n-\rho-\eta_{x}$ singular values - the zeros. The null space vectors, collected into the matrix $\nlla{}$, are defined via
\begin{equation}
\begin{array}{ccccccc}
  \wx{*} & \nlla{} &=& \nlla{}&\zero &=& \zero.\\
  \bynn  & \by{n}{\eta_{x}}    && \by{n}{\eta_{x}} & \by{\eta_{x}}{\eta_{x}} && \by{n}{\eta_{x}}
\end{array}
\end{equation}
\subitem Notice the different dimensions on the zero matrices.
This implies that
\begin{equation}
  \X{*}_{\mathcal{N}}\wx{*}\nlla{} = \paren{\A{}\,\nlla{}}^{*}\paren{\A{}\,\nlla{}} = \zero.
\end{equation}
The conclusion is the following
\begin{equation}
  \A{}\,\nlla{} = \zero
\end{equation}
where the final zero matrix has size $\eta_{x}\times \eta_{x}$.
\end{enumerate}

%%%
\subsection{Demonstration}
Let's walk through the proof and apply the methodology to a sample matrix. For 
\begin{equation}
  \A{} = \matrixb
\end{equation}
the matrix has $m=3$ rows, $n=2$ columns and has rank $rho=1$. The minimum dimension $d=2$; the column rank deficiency is
\begin{equation}
  \eta_{x} = n - \rho = 1;
\end{equation}
the row rank deficiency is
\begin{equation}
  \eta_{y} = m - \rho = 1.
\end{equation}

The domain matrices are partitioned as follows:
\begin{equation}
  \begin{array}{lcccl}
    \U{} &=& \cublockf &=& \matrixbY, \\
    \V{} &=& \cvblockf &=& \matrixbX.
  \end{array}
\end{equation}

\textit{Step 2:} The product matrix of interest is given by this:
\begin{equation}
  \wx{*} = \W{\mathrm{U}} = 3 \mat{rr}{1&-1\\-1&1}.
\end{equation}

\textit{Step 3:} The eigenvalues of the product matrix are the zeros of the characteristic polynomial, $p(\lambda)$. This polynomial is constructed from these quantities
\begin{equation}
  \begin{split}
    \trace \paren{\W{\mathrm{U}}} & = 6;\\
    \det \paren{\W{\mathrm{U}}}& = 0.
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    p(\lambda) & = \lambda^{2} - \lambda \trace \paren{\W{\mathrm{U}}} +\det \paren{\W{\mathrm{U}}};\\
    &= \lambda^{2} - 6 \lambda = 0.
  \end{split}
\end{equation}
The two eigenvalues are these:
\begin{equation}
  \lambda(\W{\mathrm{U}}) = \lst{6,0}.
\end{equation}

The singular values are the square root of the ordered, non-zero eigenvalues of the product matrix:
\begin{equation}
  \sigma_{1} = \sqrt{6}.
\end{equation}

\textit{Step 4:} Construct the $\sig{}$ matrix. It has the same size as the target matrix and the singular values are ordered on the diagonal:
\begin{equation}
  \sig{} = \sigmab.
\end{equation}

\textit{Step 5:} Construct $\V{}_{}$, the range space portion of the domain matrix. Equation \eqref{eq:proofs:a} becomes
\begin{equation}
\begin{array}{ccccc}
  \wx{*}&\rnga{} &=& \rnga{}&\ess{},\\
  \mat{rr}{3&-3\\-3&3}&\mat{r}{1\\-1} &=& \mat{r}{1\\-1} & \mat{c}{6}.
\end{array}
\end{equation}

%%%
\section{Implications}
Relating the range spaces of the domain matrices to the spaces of the target matrix:
\begin{equation}
  \begin{array}{rclcl}
     \rng{U} &=& \brnga{}  &=& \rnlla{*}^{\perp},\\
     \rng{V} &=& \brnga{*} &=& \rnlla{}^{\perp},\\
     \nll{U} &=& \rnlla{*}^{\perp} &=& \brnga{*},\\
     \nll{V} &=& \rnlla{}^{\perp} &=& \brnga{}.\\
  \end{array}
\end{equation}

Show the proofs

\endinput