\chapter{\label{ch:archetype modal}Modal Example}

\section{\label{sec:modal problem}Modal Approximation}  %    S    S    S    S    S    S    S    S    S
The following example represents a problem in linear regression. A sequence of $m$ data points $\paren{x_{k}, T_{k}}$,  $k=1\colon m$  is are recorded. The goal is to find the best approximation to a straight line. The \emph{trial function} is
  \begin{equation*}   %  =   =   =   =   =
    y(x) = a_{0} + a_{1} x .
    \label{eq:lr trial}
  \end{equation*}
The residual errors are the difference between the measurements and predictions:\\
\begin{center}
residual error$_{k}$ = measurement$_{k}$ -- prediction$_{k}$.
\end{center} 
 More formally the residual error is
  \begin{equation*}   %  =   =   =   =   =
  %\begin{split}
    r_{k} = T_{k} - y(x_{k}), \quad k=1\colon m.
    %\label{eq:}
  %\end{split}
  \end{equation*}
From this springs the \emph{merit function}, the target of minimization,
  \begin{equation}   %  =   =   =   =   =
  \begin{split}
    M(a) 
      &= \sum_{k=1}^{m} r_{k}^{2} \\
      &= \sum_{k=1}^{m} \paren{\text{measurement}_{k} - \text{prediction}_{k}}^{2} \\
      &= \sum_{k=1}^{m} \paren{T_{k} - y\paren{x_{k}}}^{2} \\
      &= \sum_{k=1}^{m} \paren{T_{k} - a_{0} - a_{1} x_{k}}^{2}
    \label{eqn:merit}
  \end{split}
  \end{equation}
The least squares solution $a_{LS}$ is formally defined as 
  \begin{equation*}   %  =   =   =   =   =
  %\begin{split}
    a_{LS} = \lst{a \in \cmplx{2} \colon \normts{y(x_{k}) - a_{0} - a_{1} x_{k}} \text{ is minimized} }.
    %\label{eq:}
  %\end{split}
  \end{equation*}
The solution satisfies
  \begin{equation}   %  =   =   =   =   =
  %\begin{split}
    \nabla M( a )|_{a_{LS}} = 0 .
    \label{eq:gradient lr}
  %\end{split}
  \end{equation}

\section{\label{sec:bevington example}Bevington Example}  %    S    S    S    S    S    S    S    S    S

To provide a common reference, see the example in Bevington \cite[ch 6]{Bevington}. The data is summarized below in table \ref{tab:bevington data and results}. The problem involves temperature measurements $T_{k}$ made at position $x_{k}$ on a bar in contact with two heat baths (Dirichlet boundary conditions). A conceptualization is shown in figure \ref{fig:bar}. Arrowheads on the bottom show the nine locations where the temperature is measured.

In the ideal linear case, the temperature at the endpoints matches the temperature of the baths, $T(x=0) = 0$ and $T(x = 10) = 100$ which describes a line with intercept $a_{0} = 0^{\circ}$C and slope $a_{1} = 10^{\circ}$C/cm. Such an expectation is a crude quality measure, a ``sniff test''.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[ width = 3in ]{\pathgraphics "bevington classic"/bar} 
   \caption[Measuring the temperature of a bar.]{Measuring the temperature of a bar held between two constant-temperature heat baths.}
   \label{fig:bar}
\end{figure}

\subsection{Problem Statement}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS
Muddled conceptions are wellsprings for muddled execution. Success in dealing with complicated problems in least squares flows from being able to see the problem cleanly; a good practice is to start with a table specifying the problem of interest, such as table \ref{tab:bevington inputs}.

The first entry is the \emph{trial function} which defines the functional form to be applied to the data. As the name indicates, this function is an initial guess. Whether or not Nature has chosen this model remains to be seen.

The \emph{merit function} is created by inserting the trial function into \eqref{eqn:merit}. This function is the target of minimization and can provide a crude check on the solution. Given a candidate solution $a_{*}$, compute the value of $M(a_{*})$. The least squares solution has the property that $M(a_{*})$ has minimum value in the neighborhood of $a_{*}$. If the solution is perturbed, one must have $M(a_{*}) < M(a_{*} + \delta a)$. When you are developing and refining least squares algorithms, you may see that the computed solution $a_{*}$ changes. For overdetermined problems, the solution is unique and comparing the values of the merit function helps discriminate solutions. In a later section figure \ref{fig:bevington merit function} will demonstrate this behavior.

The \emph{measurements} define the quantities to be measured. It seems an obvious step, but more complex models may have ambiguities start here.

\emph{Results}, or fit parameters, define the quantities to be computed using the least squares algorithm. Together with the trial function, and the measurements, we now have a clear idea of what will be measured and what will be computed.

The \emph{residual error} specifies the difference between measurement and prediction at each point. A simple matter, apparent in the merit function, it is helpful to write it out, particularly for those who may be using the results and not intimate with the derivation.

The \emph{system matrix} describes the measurement apparatus and contains the dependent variables. In this example we have $m=9$ rows (measurements), $n=2$ columns (fit parameters) and a matrix rank $\rho = 2$ (full column rank and overdetermined).

The \emph{linear system} shows the application of the trial function to every measurement. It's a good idea to keep this image in mind.

The \emph{ideal solution} is an infrequent visitor which helps provide a rough measure of quality. Caution is required, though. The ideal solution typically represents a concatenation of miracles which Nature may avoid. In this example, the ideal solution assumes magic barriers which absorb no heat, a bar of exact length, thermometers with exact measurements, heat baths at exact temperatures, no interaction with the local environment, etc. The hope is that systematic effects will be negligible and random effects will have 0 mean.

  \begin{table}[h]  %  T A B L E
    \caption{Problem statement for linear regression.}
    \begin{center}
      \begin{tabular}{lll}
        %
        \bf{trial function} & $T(x) = a_{0} + a_{1} x$ \\
        \bf{residual error} & $r_{k} = T_{k} - a_{0} - a_{1} x$ & $^{\circ}$C \\
        \bf{merit function} & $M(a) = \sum_{k=1}^{m}\paren{T_{k} - a_{0} - a_{1} x}^{2}$ \\
        \bf{measurements}   & $x_{k}$, $k=1\colon m$ & position, cm \\
                            & $T_{k}$, $k=1\colon m$ & temperature, $^{\circ}$C \\
        \bf{results}        & $a_{0} \pm \eps_{0}$ & intercept, $^{\circ}$C \\
                            & $a_{1} \pm \eps_{1}$ & slope, $^{\circ}$C / cm \\
                            %& \multicolumn{1}{c}{$\A{}$} & \multicolumn{1}{c}{$x$} & \multicolumn{1}{c}{$\A{}$} \\
        \bf{system matrix}  & $\A{} \in \real{9 \times 2}_{2}$ \\
        \bf{linear system}  & $\mat{cc}{1 & x_{1}  \\ \vdots & \vdots \\ 1 & x_{m}} 
                               \mat{c}{a_{0} \\ a_{1}} = 
                               \mat{c}{T_{1} \\ \vdots \\ T_{m}}$ \\
        \bf{ideal solution} & $\mat{c}{a_{0}\\a_{1}} = \mat{c}{0\\10}$ \\
        %
      \end{tabular}
    \end{center}
  \label{tab:bevington inputs}
  \end{table}%

The next phase is to gather and record the data as shown in table \ref{tab:bevington data and results}. Discussion of significant digits in the input data is deferred.
\begin{table}[h]
	\caption{Raw data and results.}
	\begin{center}
		\begin{tabular}{rcc|rr@{.}l}
		  %
		  & \multicolumn{2}{c}{Input} &  \multicolumn{3}{c}{Output} \\
		  %
		  $k$ & $x_{k} (cm) $ & $T_{k} (^{\circ}C)$ & $T\paren{x_{k}} (^{\circ}C)$ & \multicolumn{2}{c}{$r_{k} (^{\circ}C)$} \\\hline
		  %
			 1 & 1 & 15.6 & 14.2222 & --1 & 37778 \\
			 2 & 2 & 17.5 & 23.6306 &   6 & 13056 \\
			 3 & 3 & 36.6 & 33.0389 & --3 & 56111 \\
			 4 & 4 & 43.8 & 42.4472 & --1 & 35278 \\
			 5 & 5 & 58.2 & 51.8556 & --6 & 34444 \\
			 6 & 6 & 61.6 & 61.2639 & --0 & 336111 \\
			 7 & 7 & 64.2 & 70.6722 &   6 & 47222 \\
			 8 & 8 & 70.4 & 80.0806 &   9 & 68056 \\
			 9 & 9 & 98.8 & 89.4889 & --9 & 31111 \\
		  %
		\end{tabular}
	\end{center}
	\label{tab:bevington data and results}
\end{table}%

\subsection{\label{ssec:Normal Equations via Calculus}Normal Equations via Calculus}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS
In \S 6.4, Bevington solves the problem by applying calculus to the final form in \eqref{eqn:merit}, effectively solving \eqref{eq:gradient lr}. Introducing the notation
  \begin{equation*}   %  =   =   =   =   =
       \partial_{j} M (a_{0}, a_{1}) = \frac{\partial M(a_{0}, a_{1})}{\partial a_{j}}
  \end{equation*}
the simultaneous equations to solve are
\begin{equation*}
  \begin{split}
    &-2 \sum_{k=1}^{m} \paren{T_{k} - a_{0} - a_{1}x_{k}} \phantom{x_{k}} = 0, \\
    &-2 \sum_{k=1}^{m} \paren{T_{k} - a_{0} - a_{1}x_{k}} x_{k}  = 0.
  \end{split}
\end{equation*}
Distributing the summation operators creates a more revealing form  \\[-5pt]
  \begin{equation*}   %  =   =   =   =   =
  \begin{array}{lclclcl}
    \sum\limits_{k=1}^{m} T_{k}       & = & a_{0} \sum 1     & + & a_{1} \sum x_{k}, \\
    \sum\limits_{k=1}^{m} T_{k} x_{k} & = & a_{0} \sum x_{k} & + & a_{1} \sum x_{k}^{2}, 
  \end{array}
  \end{equation*}
where summation from 1 to $m$ is implied. (Therefore $\sum 1 = m$.) The minimization criteria are
now recast as the linear system
  \begin{equation}   %  =   =   =   =   =
      \mat{ll}{\sum 1 & \sum x_{k} \\ \sum x_{k} & \sum x_{k}^{2}}
      \mat{c}{ a_{0} \\ a_{1} } = 
      \mat{l}{\sum T_{k} \\ \sum T_{k} x_{k}}.
    \label{eq:modal matrix eq}
  \end{equation}

The solution can be written immediately. Defining the determinant
  \begin{equation*}   %  =   =   =   =   =
  %\begin{split}
    \Delta = m \sum x_{k}^{2} - \paren{\sum x_{k}}^{2},
    %\label{eq:}
  %\end{split}
  \end{equation*}
the matrix inverse is
  \begin{equation}   %  =   =   =   =   =
  %\begin{split}
    \mat{cl}{m & \sum x_{k} \\ \sum x_{k} & \sum x_{k}^{2}}^{-1} = \Delta^{-1} 
    \mat{lc}{\ps \sum x_{k}^{2} & -\sum x_{k} \\ -\sum x_{k} & m } .
    \label{eq:bevington matrix inverse}
  %\end{split}
  \end{equation}
The solution to equation \eqref{eq:modal matrix eq} is the matrix product
  % = =  e q u a t i o n
  \begin{equation*}
    \mat{c}{a_{0} \\ a_{1} } = \Delta^{-1}
    \bl{
    \mat{ll}{\ps \sum x_{k}^{2} & -\sum x_{k} \\ -\sum x_{k} & \ps \sum 1}
    \mat{l}{\sum T_{k} \\ \sum T_{k} x_{k}}
    }
    \label{eqn:bevington solution product}
  \end{equation*}
  % = =
Compare the final results to Bevington's equations 6--17:
  \begin{equation*}
  \begin{split}
    a_{0} &= \Delta^{-1} \paren{\sum x_{k}^{2}\sum T_{k} - \sum x_{k}\sum T_{k} x_{k}}, \\
    a_{1} &= \Delta^{-1} \paren{m \sum T_{k} x_{k} - \sum x_{k}\sum T_{k}}.
  \label{eqn:bevington soln}
  \end{split}
  \end{equation*}

Bevington's \S 6--5 is a succinct explanation of error propagation. In short, measurements are inexact, therefore results will be inexact. The beauty of the method of least squares is that the error in the solution parameters can be expressed in terms of the error in the data. Measurements without uncertainties are incomplete measurements.

The computation chain begins with an estimate of the parent standard deviation which is based upon the total error:
% = =  e q u a t i o n
  \begin{equation*}
    s^{2} \approx \frac{\rtr{T}} {m-n}.
  \end{equation*}
% = =
Error contributions for individual parameters are harvested from the diagonal elements of the matrix inverse $\paren{\wx{*}}^{-1}$ in \eqref{eq:bevington matrix inverse}:
\begin{equation*}
  \begin{split}
    %
    \eps_{0}^{2} &= \frac{r^{\mathrm{T}}r}{\Delta\paren{m-n}} \sum x_{k}^{2} \\
    \eps_{1}^{2} &= \frac{r^{\mathrm{T}}r}{\Delta\paren{m-n}} \sum 1 \\
    %
  \end{split}
  \label{eqn:bevington error terms}
\end{equation*}
  
\section{\label{sec:numerical results}Numerical Results}  %    S    S    S    S    S    S    S    S    S

Results are stated in two forms. The first is an integer form free of numerical errors inherent in binary representations with finite length. This liberates one from trying to determine if errors are in the algorithm or in machine arithmetic. To aid debugging, intermediate results are also provided.

The second form represents the answer which would be provided to a customer: the fit parameters and associated errors quoted with the proper amount of significant digits.
 
\subsection{Exact Form}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS
Exact results for the fit parameters are error follow. The product matrix in \eqref{eq:modal matrix eq} is 
  \begin{equation*}   %  =   =   =   =   =
  %\begin{split}
    \wx{*} = \mat{rr}{ 9 & 45 \\ 45 & 285 },
    %\label{eq:}
  %\end{split}
  \end{equation*}
with determinant
  \begin{equation*}   %  =   =   =   =   =
  %\begin{split}
    \Delta = \det\paren{\wx{*}} = 540.
    %\label{eq:}
  %\end{split}
  \end{equation*}
The inverse of this matrix, \eqref{eq:bevington matrix inverse}, is
  \begin{equation*}   %  =   =   =   =   =
  %\begin{split}
    \paren{ \wx{*} }^{-1} = \Delta^{-1} \mat{rr}{ 285 & -45 \\ -45 & 9 }.
    %\label{eq:}
  %\end{split}
  \end{equation*}
The solution vector, \eqref{eqn:bevington soln}, is
  \begin{equation*}   %  =   =   =   =   =
  %\begin{split}
      a = \mat{c}{a_{0} \\ a_{1}} = \frac{1}{360} \bl{\mat{c}{1733 \\ 3387}}.
    %\label{eq:}
  %\end{split}
  \end{equation*}
The residual error vector is $r = \wx{*} a - \A{*}T$
% = =  e q u a t i o n
  \begin{equation*}
    %\begin{split}
      r = \frac{1}{360}
          \mat{r}{-496 \\ 2207 \\ -1282 \\ -487 \\ -2284 \\ -121 \\ 2330 \\ 3485 \\ -3352},
    %\end{split}
    %\label{eqn:}
  \end{equation*}
% = =
making the total error
% = =  e q u a t i o n
  \begin{equation*}
    %\begin{split}
      \rtr{T} = \frac{1\,139\,969}{3600}.
    %\end{split}
    %\label{eqn:}
  \end{equation*}
% = =
The uncertainties are then
  \begin{equation*}   %  =   =   =   =   =
    \eps = \mat{c}{\eps_{0} \\ \eps_{1}} = \paren{360 \sqrt{35}}^{-1} \bl{\mat{r}{ 108\,297\,055 \\ 3\,419\,907}}.
  \end{equation*}

\subsection{Computed Form}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS
The results in the previous section is a debugging tool. This section deals with formats appropriate for formal reporting.
One way to quote numbers with uncertainties is using the $\pm$ (plus -- minus) notation:
% = =  e q u a t i o n
  \begin{equation*}
    \begin{array}{rclclll}
      a_{0} & = & 4.8  & \pm & 4.9  & \text{intercept} & ^{\circ} \text{C} , \\
      a_{1} & = & 9.41 & \pm & 0.87 & \text{slope}     & ^{\circ} \text{C / cm} .
    \end{array}
    \label{eq:soln vector}
  \end{equation*}
% = =
An alternative presentation uses parentheses:
% = =  e q u a t i o n
  \begin{equation*}
    \begin{array}{rclll}
      a_{0} & = & 4.8 \paren{4.9}   & \text{intercept} & ^{\circ} \text{C} , \\
      a_{1} & = & 9.41 \paren{0.87} & \text{slope}     & ^{\circ} \text{C / cm}.
    \end{array}
    %\label{eqn:}
  \end{equation*}
% = =
The total error is $\rtr{T} \approx 317.$

The uncertainty determines the number of significant digits. Common practice quotes the first two digits in the uncertainty; the location of these two digits determines the number of digits in the solution. The double precision computations are
  \begin{equation*}   %  =   =   =   =   =
    \begin{array}{rll}
      \eps_{1} &= 0.86\mg{83016476563611} & \text{rounded to 0.87}, \\
       a_{1}   &= 9.40\mg{8333333333333}  & \text{rounded to 9.41}.
    %\label{eq:}
   \end{array}
  \end{equation*}

At this point the model can be explored and evaluated. If the model is not acceptable, another trial function can be posed. Otherwise, the trial function becomes the solution function and is stated with error:
  \begin{equation*}   %  =   =   =   =   =
  \begin{split}
    T(x)         & = a_{0} + a_{1} x, \\
    \eps_{T}^{2}(x) & = \eps_{0}^{2} + x^{2} \eps_{1}^{2} + a_{1}^{2} \eps_{x}^{2} , \\
    %\label{eq:}
  \end{split}
  \end{equation*}
which allows for interpolation and extrapolation. What happens when the solution is extrapolated to the heat baths? The expected answers are $0^{\circ}$C at 0 cm, and $100^{\circ}$C at 10 cm:
  \begin{equation*}   %  =   =   =   =   =
  \begin{split}
      T \paren{0} &= \paren{ 4.8 \pm 4.9 }^{\circ}\mathrm{C}, \\
      T \paren{10} &= \paren{ 99. \pm 10. }^{\circ}\mathrm{C}.
    %\label{eq:}
  \end{split}
  \end{equation*}
  
One final thought. The method of least squares minimizes the sums of the squares of the residual errors. And in linear regression, the sum of these residuals must be 0. That is,
  \begin{equation*}   %  =   =   =   =   =
  %\begin{split}
    \sum_{k=1}^{m} r_{k} = 0 .
    %\label{eq:}
  %\end{split}
  \end{equation*}
This can be an quick method for evaluating solutions and data sets. Given the data and the solution parameters $a$ a quick Python or Mathematica script can compute and sum the residuals. If a data point is omitted, the sum will not be 0. If the parameters are misquoted, the sum will not be 0. If a data point is corrupted, the sum will not be 0. Or if the solutions are for another data set, the sum will not be 0. The 0 test is simple and powerful.

\section{Visualization}  %    S    S    S    S    S    S    S    S    S

\subsection{Seeing the Solution}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS

Numbers tell a story and plots bring a story to life. Besides have a power and immediate impact, the plots are often a defense against a wide host of problems.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[ width = 4.0in ]{\pathgraphics "bevington classic"/"red bars"} 
   \caption[Solution plotted against data with residual errors shown in red.]{Solution plotted against data with residual errors shown in red.}
   \label{fig:bevington soln v data}
\end{figure}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[ width = 4.0in ]{\pathgraphics "bevington classic"/"residuals"} 
   \caption[Scatter plot of residual errors.]{Scatter plot of residual errors.}
   \label{fig:bevington residuals}
\end{figure}

\subsection{Digger Deeper}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[ width = 4.0in ]{\pathgraphics "bevington plus"/"merit bullseye"} 
   \caption[The merit function.]{The merit function in \eqref{eqn:merit} showing the solution (white cross) and three error bands in yellow.}
   \label{fig:bevington merit function}
\end{figure}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[ width = 4.0in ]{\pathgraphics "bevington plus"/"merit fancy"} 
   \caption[Another look at the merit function.]{Another look at the merit function in \eqref{eqn:merit} showing the uncertainty parameters as elliptic radii.}
   %\label{fig:example}
\end{figure}

  \begin{table}[t]  %  T A B L E
    \caption{Results for linear regression.}
    \begin{center}
      \begin{tabular}{lll}
        %
        \bf{fit parameters} & $a_{0}$ & intercept, $^{\circ}$C \\
                            & $a_{1}$ & slope, $^{\circ}$C / cm \\
        \bf{solution function} & $T(x) = a_{0} + a_{1} x$ & $^{\circ}$C \\
        \bf{solution error} & $\eps_{T}^{2}(x) = \eps_{0}^{2} + x^{2} \eps_{1}^{2} + a_{1}^{2} \eps_{x}^{2} $ &$^{\circ}$C \\
        \bf{computed solution} & $\mat{c}{a_{0}\\a_{1}} = \mat{c}{4.8\\9.41} \pm \mat{c}{4.9\\0.87}$ \\
        \bf{ideal solution} & $\mat{c}{ \tilde{a}_{0} \\ \tilde{a}_{1} } = \mat{c}{0\\10}$ \\[5pt]%\arrayrulecolor{medgray}\hline
       %\bf{residual error} & $r^{\mathrm{T}}r \approx 317$ & temperature$^{2}$, $(^{\circ}C)^{2}$ \\
        \bf{problem statement} & table \ref{tab:bevington inputs} \\
        \bf{input data}        & table \ref{tab:bevington data and results} \\
        \bf{plots}          & figure \ref{fig:bevington soln v data} & data and solution \\
                            & figure \ref{fig:bevington residuals} & residual errors \\
                            & figure \ref{fig:bevington merit function} & merit function \\
        %
      \end{tabular}
    \end{center}
  \label{tab:bevington solution}
  \end{table}%
  
\subsection{Seeing the Uncertainty}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS
How should one interpret the uncertainties in slope and intercept? First understand the concept of distribution of errors.

\begin{table}[htbp]  % + + + + T A B L E
    \caption{The solution parameters expressed as normal distributions.}
    \begin{center}
        \begin{tabular}{cc}
            %
            intercept & slope \\
            %
            \includegraphics[ width = 2.25in ]{\pathgraphics "bevington classic"/"needle intercept"} &
            \includegraphics[ width = 2.25in ]{\pathgraphics "bevington classic"/"needle slope"} \\
            %
            $f\paren{x;a_{0},\eps_{0}} = \frac{1}{\eps_{0} \sqrt{2\pi}} e^{-\frac{(x - a_{0})^{2}}{2\eps_{0}^{2}}}$ &
            $f\paren{x;a_{1},\eps_{1}} = \frac{1}{\eps_{1} \sqrt{2\pi}} e^{-\frac{(x - a_{1})^{2}}{2\eps_{1}^{2}}}$ \\
            %
            %
            %
        \end{tabular}
    \end{center}
    %\label{default}
\end{table}%

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[ width = 3.5in ]{\pathgraphics "bevington plus"/"whiskers"} 
   \caption[Whisker plot showing 250 randomly sampled solutions.]{Whisker plot showing 250 randomly sampled solutions.}
   %\label{fig:example}
\end{figure}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[ width = 3.5in ]{\pathgraphics "bevington plus"/"scatter blue"} 
   \caption[Scatter plot showing sampling of solutions.]{Scatter plot showing sampling of solutions.}
   %\label{fig:example}
\end{figure}

  \begin{equation*}   %  =   =   =   =   =
    f\paren{x;\mu,\sigma} = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}
  \end{equation*}

  \begin{table}[htbp]  %  T A B L E
    \caption{Comparing samples to ideal normal distribution.}
    \begin{center}
      \begin{tabular}{cccr@{.}lr@{.}lr@{.}l}
        %
        &&&&& \multicolumn{2}{c}{(actual)} & \multicolumn{2}{c}{(ideal)} \\
        %
        ring & count & area &  \multicolumn{2}{c}{density} & \multicolumn{2}{c}{cumulative} & \multicolumn{2}{c}{limit} \\\hline
        %
        1 &  88 & 1 & 64 & 83\% & 64 & 83\% & 68 & 27\% \\
        %
        2 & 115 & 3 & 28 & 24\% & 93 & 07\% & 95 & 45\% \\
        %
        3 &  41 & 5 & 6 & 41\% & 99 & 16\% & 99 & 73\%\\
        %
        4 &   6 & 5 & 0 & 88\% & 100 & 0\% & 99 & 99\%\\
        %
      \end{tabular}
    \end{center}
  %\label{tab:?}
  \end{table}%

\endinput