\chapter{Glossary}

%%
\section*{Alphabetics}

%%
\subsection*{A} 

\subsubsection*{adjoint matrix}
The conjugate of the transpose matrix. Since these operations commute the adjoint matrix can be written as this
\begin{equation}
  \A{*} = \overline{\A{}}^{\mathrm{T}} = \overline{\A{T}}.
\end{equation}
This definition insures that the product matrices $\A{}\A{*}$ and $\A{*}\A{}$ are positive semidefinite for complex matrices $\A{}$.
Also known as 
\begin{itemize}
\item conjugate transpose,
\item Hermitian transpose,
\item Hermitian conjugate.
\end{itemize}
Earlier texts often used adjoint in place of the more current adjugate. Matrices satisfying the property
\begin{equation}
  \A{}=\A{*}
\end{equation}
are called \textit{normal matrices}.

Example:
\begin{equation}
\mat{ccc}
{
0 & i & 1-i \\
\sqrt{2} & a & 3i
}^{*}
=
\mat{cc}
{
0  & \sqrt{2}\\
-i & \overline{a} \\
1+i& -3i
}
\end{equation}

\subsubsection*{adjugate matrix}
The transpose of the matrix of cofactors. In earlier texts this was also called the adjoint matrix. The adjugate is written this way:
\begin{equation}
  \text{adj}\paren{\A{}}=\AA^{\mathrm{T}}.
\end{equation}
With this formalism the inverse matrix can be cast as this
\begin{equation}
  \A{-1}=\frac{\text{adj}\paren{\A{}}}{\det \paren{\A{}}}.
\end{equation}

\subsubsection*{augmented matrix}
Two matrices stapled together column-wise. This allows row operations to be performed on both matrices simultaneously.

Example: augment the follow matrix with an identity matrix.
\begin{equation}
\begin{array}{ccc}
  \A{} & \to & \mat{c|c}{\A{}&\I{3}} \\
  \Aexample & \to & \mat{rr|ccc}{1 & -1 & 1 & 0 & 0\\-1 & 1 & 0 & 1 & 0\\1 & -1 & 0 & 0 & 1}
\end{array}
\end{equation}

%%
\subsection*{B} 

\subsubsection*{basic variables}
\subsubsection*{basis variables}

%%
\subsection*{C} 

\subsubsection*{characteristic polynomial}
The polynomial generated by the formula
\begin{equation}
  \det \paren{\A{}-\lambda \I{}}
\end{equation}
where $\Ac{m}$. The roots of this polynomial are the eigenvalues of the matrix $\A{}$. Two special cases are these:
\begin{equation*}
  \Ac{2},
\end{equation*}
\begin{equation}
  p(\lambda) = \lambda^{2} -\lambda     \tr (\A{}) + \det(\A{});
\end{equation}
\begin{equation}
  \Ac{3},
\end{equation}
\begin{multline}
  p(\lambda) = \lambda^{3} -\lambda^{2} \tr (\A{}) + \paren{\tr^{2}(\A{})-\tr(\A{2})}\lambda\\
   + \paren{\tr^{3}(\{A})+2\tr(\A{3})-3\tr(\A{})\tr(\A{2})
\end{multline}
\subsubsection*{codomain}
The domain of the transpose matrix.

\subsubsection*{cokernel}

\subsubsection*{cofactor}

\subsubsection*{column space}
The vector space spanned by the column vectors.

\subsubsection*{commutator}
An operator that defines the commutation relationship between matrices
$$
 \left[ \A{}, \textbf{B} \right] = \A{}\textbf{B}-\textbf{B}\A{}.
$$
If the commutator is zero, if $\left[ \A{}, \textbf{B} \right] = \zero$ then the matrices $\A{}$ and $\textbf{B}$ commute.

Example: Rotations in two dimensions commute.
\begin{equation}
  \brac{\rot, \textbf{R}\paren{\phi}} = \brac{
  \mat{rr}{ \cos \theta&-\sin \theta \\ \sin \theta&\cos \theta },\mat{rr}{ \cos \phi&-\sin \phi \\ \sin \phi&\cos \phi}} = \mat{rr}{ 0&0\\0&0}
\end{equation}

Rotations in three dimensions about different axes do not commute. 
\begin{equation}
  \begin{split}
    \textbf{R}_{x_2 x_3}\paren{\theta} \textbf{R}_{x_1 x_3}\paren{\phi} &= 
    \brac{
\mat{ccc}{
 1 & 0 & 0 \\
 0 & \cos\theta & -\sin\theta \\
 0 & \sin\theta & \cos\theta
}
,
\mat{ccc}{
 \cos \phi & 0 & -\sin \phi \\
 0 & 1 & 0 \\
 \sin \phi & 0 & \cos \phi
}
} \\
  & =
  \mat{ccc}{
   0 & \sin\theta \sin \phi & -\sin \phi+\cos\theta \sin \phi \\
  -\sin\theta \sin \phi & 0 & \sin\theta-\cos \phi \sin\theta \\
  -\sin\phi+\cos\theta \sin \phi & \sin\theta-\cos \phi \sin\theta & 0
  } \\
  & \ne
  \mat{ccc}{
   0 & 0 & 0 \\
   0 & 0 & 0 \\
   0 & 0 & 0
  }
  \end{split}
\end{equation}

%%
\subsection*{D} 

\subsubsection*{domain}
The set of points which define valid input to a function. For example the function
\begin{equation}
y(x) = \sqrt{1-x^{2}}
\end{equation}
is valid over the domain $x\in [-1,1]$.

\subsubsection*{dot product}
A projection operator. See diagram. Although the name evokes the $\cdot$ symbol often used to encode the operation, there are other forms as shown below
Given 
\begin{equation}
  x = 
  \mat{c}{
  x_1 \\
  x_2 \\
  x_3
  },
  \quad
  y = 
  \mat{c}{
  y_1 \\
  y_2 \\
  y_3
  },
\end{equation}
the dot product is
\begin{equation}
  \begin{split}
x \cdot y = \inner{x,y} = x^T y 
  &= 
  \mat{ccc}{
  x_1 & x_2 & x_3
  }
  \mat{c}{
  y_1 \\
  y_2 \\
  y_3
  }\\
  & = x_1 y_1 + x_2 y_2 + x_3 y_3 \\
  & = \sum_{j=1}^{3}{x_j y_j}
  \end{split}
\end{equation}

%%
\subsection*{E} 

\subsubsection*{EAR}
Reduction of augmented matrices by elementary matrices. The process identifies the free and basic variables and provides null space vectors.

\subsubsection*{equivalence}
Meyer, p.134

%%
\subsection*{F} 

\subsubsection*{free variables}

\subsubsection*{fundamental subspaces}

\subsubsection*{Fundamental Theorem of Linear Algebra}
This theorem describes the relationships and dimensions and dimensions of the four fundamental subspaces.
For every $ \A{} \in \cmplx{ m \times n } $,
$$
\begin{array}{ lclclcl }
\\
  \cmplx{ m } & = & \rng{\A{}} \ \oplus \  \rng{\A{}}^{ \perp } & \qquad \qquad & \cmplx{ n } & = & \rng{\A{*}} \ \oplus \  \rng{\A{*}}^{ \perp } \\[7pt]
  & = & \rng{\A{}} \ \oplus \  \nll{\A{*}} & \qquad \qquad & & = & \rng{\A{*}} \ \oplus \  \nll{\A{}} \\[7pt]
  & &  domain & & & &  image \\[5pt]
\end{array}
$$
$$
\begin{array}{ |lcl|c|lcl| }
\\
  \cmplx{ m } & = & \rng{\A{}} \ \oplus \  \rng{\A{}}^{ \perp } & \longrightarrow \A{} \longrightarrow & \cmplx{ n } & = & \rng{\A{*}} \ \oplus \  \rng{\A{*}}^{ \perp } \\[7pt]
  & = & \rng{\A{}} \ \oplus \  \nll{\A{*}} & \longleftarrow \A{*} \longleftarrow & & = & \rng{\A{*}} \ \oplus \  \nll{\A{}} \\[7pt]
  & &  domain & & & &  image \\[5pt]
\end{array}
$$

%%
\subsection*{G} 

\subsubsection*{Gell-Mann matrices}
A matrix representation of the infinitesimal generators of the group SU(3).

\subsubsection*{Generalized inverse}p
See {\it pseudoinverse}

\subsubsection*{Gram-Schmidt process}

\subsubsection*{groups}

\subsubsection*{\textbf{GL}($n$,\textbf{C})}
The group of invertible $n \times n$ complex matrices. The matrices are square with non-zero determinant. They have no zero eigenvalues.

\subsubsection*{\textbf{SL}(2,\textbf{C})}
The group of invertible $n \times n$ complex matrices with unit determinant. The matrices are square with non-zero determinant. They have no zero eigenvalues.

%%
\subsection*{H} 

\subsubsection*{Hermitian conjugate}
Example: $\mat{cc}{\sqrt{2}&1+i\\1-i&\pi}^{*}=\mat{cc}{\sqrt{2}&1+i\\1-i&\pi}$.

%%
\subsection*{I} 

\subsubsection*{idempotent}
A matrix that is unchanged after $k$ multiplications, that is
$$
\A{k} = \A{},
$$
where $k$ is the smallest integer which preserves the identity.\\

Example: $\mat{cc}{i&0\\0&i}^5=\mat{cc}{i&0\\0&i}$.

\subsubsection*{image}
All possible combinations of the columns of a target matrix. For example, consider a matrix of three column vectors $c1, c2,$ and $c3$:
\begin{equation}
  \A{} = \mat{c|c|c}{c_{1}&c_{2}&c_{3}}.
\end{equation}
The image of $\A{}$ is the set of all possible combinations of these vectors:
\begin{equation}
  \text{image}\paren{\A{}} = \alpha c_{1} + \beta c_{2} + \gamma c_{3}
\end{equation}
where the factors $\alpha, \beta$, $\gamma$ are arbitrary complex constants.

\subsubsection*{inner product}

\subsubsection*{involutary}
A matrix which is it's own inverse. The matrix $\A{}$ is involutary if and only if $\A{2} = \I{}$. Think of these matrices as a generalization of the square root of the identity.

%%
\subsection*{J} 

\subsubsection*{$\J{m}{\rho}$}
The truncated identity matrix, an identity matrix where the diagonal of ones is depleted from the bottom up. It is a square matrix of dimension $m$ with $\rho $ unit entries on the diagonal.
Example:
$$
\J{2}{1} = \mat{ccc}{1 & 0 \\ 0 & 0}
$$

\subsubsection*{Jacobian}

\subsubsection*{Jacobi relationship}

\subsubsection*{Jordan normal form}
\begin{equation}
  \A{} = 
  \mat{ccc}{
  1 & 1 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1
  }
\end{equation}


%%
\subsection*{K} 

\subsubsection*{kernel}

%%
\subsection*{M} 

\subsubsection*{magnitude}

\subsubsection*{minor}

\subsubsection*{Moore-Penrose inverse}
See {\it pseudoinverse}

%%
\subsection*{N} 

\subsubsection*{norm, vector}
A quantity to describe the length of a vector.
Examples: Given $x^T=\lst{x_1,x_2,x_3,x_4}$,
$$
\begin{array}{ lcl }
    \normo{x}   &=& \abs{x_1}+\abs{x_2}+\abs{x_3}+\abs{x_4} \\[5pt]
    \normt{x}   &=& \sqrt{x_1^2+x_2^2+x_3^2+x_4^2} \\[5pt]
    \norminf{x} &=& \mx \lst{\abs{x_1},\abs{x_2},\abs{x_3},\abs{x_4}} \\
\end{array}
$$
The $2-$norm is the familiar Pythagorean theorem.

\subsubsection*{norm, matrix}
Vector norms can induce norms on matrices. A length measurement.
The most prevalent induced matrix norms are these
\begin{equation}
\begin{array}{ lcl }
    \normo{\A{}}   &=& \max_{row=1,m}\sum_{c=1,n}{\abs{\A{}_{row,c}}} \\[5pt]
    \normt{\A{}}   &=& \abs{\sigma_{1}} \\[5pt]
    \norminf{\A{}} &=& \max_{col=1,n}\sum_{r=1,m}{\abs{\A{}_{r,col}}} \\
\end{array}
\end{equation}
where $\lambda_{1}$ is the largest eigenvalue.
The Frobenius norm, shown below, is not an induced norm but it is a matrix norm. For $\A{}\in\cmplx{\by{m}{n}}$
\begin{equation}
  \norm{\A{}}_F = \sqrt{\sum_{r=1}^{m}{\sum_{c=1}^{n}{\A{}_{r,c}\overline{\A{}_{r,c}}}}}
\end{equation}

\subsubsection*{normal}

\subsubsection*{normal matrix}
A matrix is normal if it commutes with its adjoint:
\begin{equation}
  \brac{ \A{}, \A{*} } = 0.
\end{equation}
A normal matrix can be diagonalized by a unitary matrix.

\subsubsection*{null space}

%%
\subsection*{O} 

\subsubsection*{orthogonal}
In general this means perpendicular. Two vectors $a$ and $b$ are orthogonal if and only if $a\cdot b=0$. A matrix $\A{}$ is orthogonal if and only if $\A{}\A{*} = \bf{I}_m $. That is if the Hermitian conjugate is the also the inverse matrix: $\A{*}=\A{-1}$.

\subsubsection*{orthonormal}
Orthogonal and normal.

\subsubsection*{outer product}
The outer product of two vectors is a rank one matrix. Given 
\begin{equation}
  x = 
  \mat{c}{
  x_1 \\
  x_2 \\
  x_3
  },
  \quad
  y = 
  \mat{c}{
  y_1 \\
  y_2 \\
  y_3 \\
  y_4
  },
\end{equation}
the outer product is
\begin{equation}
  \begin{split}
x \otimes y = x y^T = 
  \mat{c}{
  x_1 \\
  x_2 \\
  x_3
  }
  \mat{cccc}{
  y_1 & y_2 & y_3 & y_4
  }
  & = 
  \mat{c}{
  x_1 y^{\mathrm{T}}\\
  x_2 y^{\mathrm{T}}\\
  x_3 y^{\mathrm{T}}
  }\\
  &=
  \mat{cccc}{
  x_1 y_1 & x_1 y_2 & x_1 y_3 & x_1 y_4 \\
  x_2 y_1 & x_2 y_2 & x_2 y_3 & x_2 y_4 \\
  x_3 y_1 & x_3 y_2 & x_3 y_3 & x_3 y_4 \\
  } \\
  \end{split}
\end{equation}

%%
\subsection*{P} 

\subsubsection*{Pauli matrices}
A set of Hermitian, traceless, involutary matrices often defined as\begin{equation*}
  \sigma_1 = \paulia, \quad \sigma_2 = \paulib, \quad \sigma_3 = \paulic.	
\end{equation*}
They are orthonormal spanning matrices for the special unitary group of dimension 2: 
$$
SU(2) = \spn \lst{i \sigma_1,i \sigma_2,i \sigma_3}.
$$ 
Also, any matrix $\A{} \in \cmplx{2 \times 2}$ can be composed using these matrices as a basis:
\begin{equation*}
  \A{} = c \, \textbf{I}_2 + \alpha \cdot \pmb{\sigma}
\end{equation*}
where $c$ is a complex scalar and $\alpha$ is a complex vector of length 3.

\subsubsection*{Permutation matrix}
A matrix of complete permutation. Think of it as an identity matrix where the rows are reversed. Or as a matrix where the columns are reversed.
\begin{equation}
  \K{2} = \ktwo, \quad \K{3} = \kthree, \quad \K{4} = \kfour, \quad \dots
\end{equation}

\textit{Premultiplication} completely interchanges \textit{row} vectors:
\begin{equation}
  \begin{split}
   \K{n}\A{} = \mat{ccccc}
  {0 & 0 & \cdots & 0 & 1\\
   0 & 0 & \cdots & 1 & 0\\
   \vdots & \vdots & \iddots & \vdots & \vdots\\
   0 & 1 & \cdots & 0 & 0\\
   1 & 0 & \cdots & 0 & 0}
   \mat{c}
  {r_{1} \\\hline r_{2} \\\hline \vdots \\\hline r_{m}}
    =
   \mat{c}
  {r_{m} \\\hline \vdots \\\hline r_{2} \\\hline r_{1}}.
  \end{split}
\end{equation}

\textit{Postmultiplication} completely interchanges \textit{column} vectors:
\begin{equation}
  \begin{split}
  \A{}\K{m} &=    
   \mat{c|c|c|c}
  {c_{1} & c_{2} & \dots & c_{n}}
   \mat{ccccc}
  {0 & 0 & \cdots & 0 & 1\\
   0 & 0 & \cdots & 1 & 0\\
   \vdots & \vdots & \iddots & \vdots & \vdots\\
   0 & 1 & \cdots & 0 & 0\\
   1 & 0 & \cdots & 0 & 0}\\
   & = \mat{c|c|c|c}
  {c_{n} & \dots  & c_{2} & c_{1}}.
  \end{split}
\end{equation}
The permutation matrices are idemtpotent:
\begin{equation}
  \K{n}\K{n} = \I{n}.
\end{equation}

\subsubsection*{pseudoinverse}
Also {\it Moore-Penrose inverse}, {\it generalized inverse}

%%
\subsection*{R} 

\subsubsection*{rank}

\begin{itemize}
\item The number of independent rows.
\item The number of independent columns.
\item The number of nonzero singular values.
\end{itemize}
Also {\it matrix rank}
 
\subsubsection*{range}

\subsubsection*{rectangular matrix}
A matrix with an unequal number of rows and columns. While squares are rectangles, in the context of matrix algebra there is a need to distinguish between matrices which are square and those which are not. Therefore a square matrix has an equal number of rows and columns.

\subsubsection*{row space}
The row vectors are a basis for the row space.

%%
\subsection*{S} 

\subsubsection*{sabot}
The sabot matrix is an empty, shape-holding. For a target matrix $\Acc{m}{n}$ the sabot matrix is a matrix of zeros of size $m \times n$.

\subsubsection*{sigma matrix}
The sabot matrix holding the singular value matrix. This matrix has the same dimensions as the target matrix. In terms of the singular values matrix the representation is this
\begin{equation}
  \sig{}=\mat{cc}{\ess{} & \zero\\ \zero & \zero}.
\end{equation}

\subsubsection*{singular value}
The non-zero eigenvalues of the product matrices $\prdm{*}$ and $\prdmm{*}$. For a target matrix $\Acc{m}{n}_{\rho}$ the product matrices $\W{x}$ and $\W{y}$ both have $\rho$ nonzero eigenvalues. When these nonzero eigenvalues are arranged in descending order
\begin{equation}
  \begin{split}
    \sigma_{k} &= \sqrt{\lambda_{k}\paren{\W{x}}}, \quad k=1,\rho\\
               &= \sqrt{\lambda_{k}\paren{\W{y}}}
  \end{split}
\end{equation}

\subsubsection*{singular values matrix}
The full-rank diagonal matrix of ordered singular values.
\begin{equation}
  \ess{}=\mat{ccccc}{\sigma_{1}\\&\sigma_{2}\\&&\ddots\\&&&\sigma_{\rho}}
\end{equation}


\subsubsection*{singular value decomposition}
Perhaps the most powerful matrix decomposition.

\subsubsection*{span}

\subsubsection*{square matrix}
A matrix with an equal number of rows and columns.

\subsubsection*{Strang lectures}
A series of lectures on linear algebra by Professor Gilbert Strang which accompany his book. Freely available at the MIT OCW web site.

\subsubsection*{symmetric matrix}
Example: $\mat{cc}{\sqrt{2}&1+i\\3i&i\pi}^{*}=\mat{cr}{\sqrt{2}&-3i\\1-i&i\pi}$.

%%
\subsection*{T} 

\subsubsection*{transpose}
As a verb it means to interchange the rows and columns of a matrix. As a noun it refers to the matrix with rows and columns interchanged.

\subsubsection*{truncated identity matrix}
An identity matrix with some zero elements on the lower diagonal. Referenced as $\J{m}{\rho}$ where $m$ is the size of the matrix and $\rho$ is the rank. Another way to think of the matrix is as zero matrix of rank $m$ embedded with an identity matrix of dimension $r$ in the upper left-hand corner.

Example: $\J{2}{1} = \mat{cc}{1 & 0 \\ 0 & 0}$.

%%
\subsection*{X} 

\subsubsection*{$\X{}$ matrix}
The matrix in the \svdl \ which describes a complete orthonormal basis for the domain of the target matrix. A span of the row space forms the first columns and a span of the perpendicular space competes the matrix.


%%
\subsection*{Y} 

\subsubsection*{$\Y{}$ matrix}
The matrix in the \svdl \ which describes a complete orthonormal basis for the codomain of the target matrix. A span of the column space forms the first columns and a span of the perpendicular space competes the matrix.

%%
\subsection*{Z} 

\subsubsection*{Zernike polynomials}
A set of complex polynomials $V_n^m \paren{z,\overline{z}}$ orthogonal over the unit disk. The are the unique set of polynomials which are orthogonal and satisfy the rotational invariance property $$V_n^m \paren{r,\theta} = e^{ -i m \phi }V_n^m \paren{r,\theta + \phi}.$$
In practice these polynomials are expressed in terms of their real and imaginary components.

Example: \\
\begin{equation*}
  \begin{split}
V_3^1 \paren{z,\overline{z}} = z\paren{3z\overline{z}-2}; \quad 
\Re\paren{V_3^1} &= Z_3^{1}\paren{x,y} = 3x^3+3xy^2-2x, \\ 
\Im\paren{V_3^1} &= Z_3^{-1}\paren{x,y} = 3y^3+3x^2y-2y. \\   
  \end{split}
\end{equation*}

%%
\subsection*{$\Sigma$} 

\subsubsection*{$\sigma$}
A singular value. If $\lambda_j,\ j=1,\rho$ are the nonzero eigenvalues of the product matrices $\prdm{H}$ and $\prdmm{H}$ then
\begin{equation}
  \sigma_i = \sqrt{\lambda_i}.
\end{equation}

\subsubsection*{$\sig{}$ matrix}
The diagonal matrix of singular values in the singular value decomposition. This matrix inherits the shape of the target matrix and the number of nonzero elements on the diagonal equals the matrix rank.

\subsubsection*{$\sig{(+)}$}
The inverse for the $\sig{}$ matrix. Take the transpose (or Hermitian conjugate) of the $\sig{}$ matrix and invert the nonzero singular values.

%%
\section*{Numbers} 

\subsubsection*{\zero}
An \textit{array} of zeros used to pad a matrix. The shape of the array is determined from the context. For example, equation \eqref{eq:6:example} presents the matrix
\begin{equation}
\sig{(+)} =
\mat{c}{
 \textbf{S}^{ -1 } \\
 \zero
 }.
\end{equation}
Since $\sig{(+)}$ has dimension $4 \times 4$ and $\textbf{S}$ has dimension $2 \times 2$ the zero array in this instance is a $2 \times 2$ array:
\begin{equation}
  \zero = 
  \mat{cc}{
  0 & 0 \\
  0 & 0
  }.
\end{equation}

\subsubsection*{\one}
A \textit{vector} of ones which stands for $x^0$ where $x$ is a vector of length $n$. Creating a list of ones avoids the ambiguous form $0^0$. Note that 
\begin{equation}
  x \cdot \one = x^T \one = n.
\end{equation}

%%
\section*{Symbols} 

\subsubsection*{$\Im$}
The imaginary part of a complex number or function. For the complex number $z=a+ib$ the imaginary part is $\Im z=b$.

\subsubsection*{$\Re$}
The real part of a complex number or function. For the complex number $z=a+ib$ the real part is $\Re z=a$.

\subsubsection*{$\bullet$}
The bullet symbol is used a placeholder and represents a number whose value is not relevant in the context of the presentation.

\subsubsection*{$\star$}
The star symbol is used a placeholder and represents a number whose value is not relevant in the context of the presentation.

%%
\section*{Operations} 

\subsection*{$\abs{\cdot}$}
Absolute value or magnitude. For vectors $v$,
\begin{equation}
  \abs{v}=\sqrt{v\cdot v}.
\end{equation}
For complex numbers $z=a+ib$
\begin{equation}
  \abs{z}= \sqrt{z\overline{z}} = \sqrt{a^{2}+b^{2}}.
\end{equation}

\subsection*{$\norm{\cdot}$}
Vector norm or matrix norm. For a vector $v$ of length $n$ the $p-$norm is defined as
\begin{equation}
  \norm{v}_{p} = \sqrt[p]{v_{1}^{p}+v_{2}^{p}+\dots+v_{n}^{p}}.
\end{equation}

\subsection*{$\oplus$}

\subsection*{$\otimes$}
Outer product.

\endinput