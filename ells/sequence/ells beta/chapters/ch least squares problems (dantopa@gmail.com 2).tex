\chapter{\label{ch:least squares problem}Least Squares Problems}

\section{\label{sec:linear systems}Linear Systems}  %    S    S    S    S    S    S    S    S    S    S    S    S

  This story begins with the archetypal \index{matrix-vector equation} matrix--vector equation
  \begin{equation}   %  =   =   =   =   =
    \axeb .
    \label{eq:axeb}
  \end{equation}
The matrix $\A{}$ has $m$ rows, $n$ columns, and has rank $\rho$; the vector $b$ encodes $m$ measurements. The solution vector $x$ represents the $n$ free parameters in the model. In mathematical shorthand,
  \begin{equation}   %  =   =   =   =   =
    \aicmnr, \quad b \in \cmplxm, \quad x \in \cmplxn
  \label{eq:basics}
  \end{equation}
with $\cmplx{}$ representing the field of complex numbers. The matrix $\A{}$ and the vector $b$ are given, and the task is to find the vector $x$.

\subsection{\label{ssec:exact soln}$\reserr = 0$}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS
The letters in \eqref{eq:axeb} will change, but the operation remains the same: a matrix operates on an $n-$vector and returns an $m-$vector. We can think of the \index{matrix} matrix as a map from vectors of dimension $n$ to vectors of dimension $m$:
  \begin{equation*}   %  =   =   =   =   =
    \A{} \colon \cmplxn \mapsto \cmplxm .
  \end{equation*}

If the vector $b$ can be expressed a combination of the columns of the matrix $\A{}$ then there is a direct solution:
%  e q u a t i o n
\begin{equation*}
  %\begin{split}
    \axeb \quad \Longrightarrow \quad x_{1} a_{1} + \cdots + x_{n} a_{n} = b
  %\end{split}
%\label{eq:}
\end{equation*}
%  e q u a t i o n
and the residual error \index{residual error} vanishes:
  \begin{equation*}   %  =   =   =   =   =
   %\begin{split}
      \axmb = \zero
   %\end{split}
 %\label{eq:}
  \end{equation*}
where the zero vector $\zero$ is a list of $m$ zeros. The total error, the norm of this vector, is 0.

For the problem where the system matrix $\A{}$ is the identity matrix $\I{2}$:
  \begin{equation*}   %  =   =   =   =   =
   %\begin{split}
      \idtwo \xtwo = \mat{c}{b_{1} \\ b_{2}},
   %\end{split}
 %\label{eq:}
  \end{equation*}
the solution is
  \begin{equation*}   %  =   =   =   =   =
   %\begin{split}
      \xtwo = \mat{c}{b_{1} \\ b_{2}} ;
   %\end{split}
 %\label{eq:}
  \end{equation*}
there is no residual error 
  \begin{equation*}   %  =   =   =   =   =
   %\begin{split}
      \A{} x - b = \zerotwo .
   %\end{split}
 %\label{eq:}
  \end{equation*}

\subsection{\label{ssec:no exact soln}$\reserr > 0$}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS
But what happens when the vector $b$ is \emph{not} in the column space of the matrix $\A{}$? The solution criteria must relax. Instead of seeking zero residual error, seek minimal residual error
  \begin{equation*}   %  =   =   =   =   =
  %\begin{split}
    r = \axmb .
    %\label{eq:}
  %\end{split}
  \end{equation*}
Instead of a perfect solution, ask for the best solution. One such class of solutions are least squares solutions\index{leasts squares!solutions}.

\section{\label{sec:lss}Least Squares Solutions}  %    S    S    S    S    S    S    S    S    S    S    S    S    S    S
In both the zonal and modal approximations, the goal is to minimize the residual error
  \begin{equation*}   %  =   =   =   =   =
   %\begin{split}
      \norm{r} = \reserr.
   %\end{split}
 %\label{eq:}
  \end{equation*}
This work explores the minimal solutions under the $2-$norm, the familiar norm of Pythagorus:
  \begin{equation*}   %  =   =   =   =   =
   %\begin{split}
      \normt{ \mat{c}{x_{1} \\ x_{2}} } = \sqrt{x_{1}^{2} + x_{2}^{2}}.
   %\end{split}
 %\label{eq:}
  \end{equation*}

Let's construct a sample problem with $\norm{\axmb} > 0$ by modifying the previous example:
  \begin{equation}   %  =   =   =   =   =
      \mat{cc}{1 & 0 \\ 0 & 0} \xtwo = \mat{c}{b_{1} \\ b_{2}} .
      \label{eq:simple case}
  \end{equation}
When $b_{2} \ne 0$ there is no solution with $\reserr > 0$. Consider the solutions given by
  \begin{equation}   %  =   =   =   =   =
      x_{*} = \mat{c}{ x_{1} \\ 0 } ;
      \label{eq:xstar}
  \end{equation}
the error is 
  \begin{equation}   %  =   =   =   =   =
   %\begin{split}
      \A{} x_{*} - b = -\mat{c}{0 \\ b_{2}}
   %\end{split}
  \label{eq:simple case solution}
  \end{equation}
which has a norm $\norm{\A{}x_{*} - b} = \abs{b_{2}}$, plotted in figure \ref{fig:v graph}. This is the least possible error for the problem and \eqref{eq:xstar} is the best solution. In this light, the transition from an exact solution to an inexact solution is natural.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[ width = 4in ]{../pdf/"least squares"/"v graph"} 
   \caption{The least squares solution \eqref{eq:simple case solution} for \eqref{eq:simple case}} .
   \label{fig:v graph}
\end{figure}

Think of the solutions to the linear system
  \begin{equation*}   %  =   =   =   =   =
  %\begin{split}
    \axeb
    %\label{eq:}
  %\end{split}
  \end{equation*}
as being described by the inequality
  \begin{equation*}   %  =   =   =   =   =
  %\begin{split}
    \normt{\axmb} \ge 0.
    %\label{eq:}
  %\end{split}
  \end{equation*}
In some cases the equality is attained.

Least squares solutions are classified by the interpretation of the output. In the first case, \emph{zonal approximation}, the output represents data at a physical zone, a point or a region. In the second case, \emph{modal approximation}, the output represents an amplitude, a contribution for a mode. Basic examples follow.

\subsection{\label{ssec:zonal approx}Zonal Approximation}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS
Consider the vector field $F$ described by the gradient of a scalar field $\phi$.
  \begin{equation*}   %  =   =   =   =   =
    F = \nabla \phi
  \end{equation*}

\subsubsection{\label{ssec:zonal solution}Zonal Problem}  %  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS
In practice one measures the vector field and solves the inverse problem. The input and outputs are represented in \ref{fig:sticks}. The physical field is $\phi(x)$, $0\le x \le 2$, the approximation is $\varphi_{x_{k}}$, $k=0,1,2$. For a cleaner presentation let $\varphi_{x_{k}} \rightarrow \varphi_{k}$. The first measurement $x_{1}$ represents the potential change between $\phi(0)$ and $\phi(1)$; the second measurement $x_{2}$ the change between $\phi(1)$ and $\phi(2)$.
  \begin{equation*}   %  =   =   =   =   =
    \begin{split}
      \varphi_{1} - \varphi_{0} &\approx \delta_{1} \\
      \varphi_{2} - \varphi_{1} &\approx \delta_{2}
    %\label{eq:}
    \end{split}
  \end{equation*}
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
%   \includegraphics[ width = 3in ]{../eps/theory/ftola/"sticks and ideal curve gimp"} 
%   \includegraphics[ width = 3in ]{../eps/"least squares"/"sticks and ideal curve"} \\
    \includegraphics[ width = 3in ]{../pdf/"least squares"/"sticks and ideal"} 
%   \includegraphics[ width = 3in ]{../eps/"least squares"/"sticks and ideal"} 
   \caption[Scalar function $\phi$ and approximations.]{Scalar function $\phi(x)$ (curve) and approximation $\varphi_{k}$ (sticks).}
   \label{fig:sticks}
\end{figure}

The system matrix 
  \begin{equation*}   %  =   =   =   =   =
   %\begin{split}
      \A{} =     \mat{rrr}{ 
      -1 & 1 & 0 \\
       0 & -1 & 1 } \in \real{2 \times 3}_{2}.
   %\end{split}
 %\label{eq:}
  \end{equation*}
There are $m = 2$ measurements, $n = 3$ measurement locations, and the matrix rank is $\rho = 2$. Because the rank is less than the number of columns, $\rho < n$, this problem is \emph{underdetermined} \index{underdetermined system}.

The linear system is
  \begin{equation}   %  =   =   =   =   =
    \mat{rrr}{ 
      -1 & 1 & 0 \\
       0 & -1 & 1 }
    \mat{c}{ \varphi_{0} \\ \varphi_{1} \\ \varphi_{2} }
    =
    \mat{c}{ \delta_{1} \\ \delta_{2} } .
    \label{eq:zonalls}
  %\end{split}
  \end{equation}

\subsubsection{\label{ssec:zonal solution}Zonal Solution}  %  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS

The solutions for the linear system in \eqref{eq:zonalls} which minimize $\normt{\axmb}$ are
  \begin{equation*}   %  =   =   =   =   =
    \mat{c}{ \varphi_{0} \\ \varphi_{1} \\ \varphi_{2} } =
    \bl{\mat{rr}{ -2 & -1 \\ 1 & -1 \\  1 & 2 }
    \mat{c}{ \delta_{1} \\ \delta_{2} }} + \gamma
    \rd{\mat{c}{ 1 \\ 1 \\ 1 }}, \qquad \gamma \in \cmplx{} .
  \end{equation*}
The color blue represents range space vectors, red null space vectors. In this way, the fundamental spaces spring to life.

There is a continuum of solutions due to the fact that
  \begin{equation*}   %  =   =   =   =   =
      \Ax = \A{} \paren{x + \alpha \rd{\mat{c}{ 1 \\ 1 \\ 1 } } }.
  \end{equation*}
An easy demonstration is to write
  \begin{equation*}   %  =   =   =   =   =
    \A{} \rd{\mat{c}{ 1 \\ 1 \\ 1 }} 
      = \mat{rrr}{ -1 & 1 & 0 \\ 0 & -1 & 1 } \rd{\mat{c}{ 1 \\ 1 \\ 1 }} 
      = \mat{c}{ 0 \\ 0 } .
  \end{equation*}

\subsection{\label{ssec:modal approx}Modal Approximation}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS
\subsubsection{\label{ssec:modal problem}Modal Problem}  %  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS
In the modal approximation, the user first selects a set of basis functions to describe measurements. Popular basis functions include orthogonal polynomials, trigonometric functions, or monomials. For example, a linear regression implies a basis set of two elements: a constant function, and a linear function. This leads to the familiar equation for a straight line:
  \begin{equation*}   %  =   =   =   =   =
    y(x) = a_{0} + a_{1} x
  \end{equation*}
The $n=2$ parameters represent the intercept $\paren{a_{0}}$, and the slope $\paren{a_{1}}$; each of the $m$ measurements represents a straight line:
  \begin{equation*}   %  =   =   =   =   =
   \begin{split}
     a_{0} + a_{1} x_{1} &= y_{1} \\
       & \ \, \vdots \\
     a_{0} + a_{1} x_{m} &= y_{m} .
    %\label{eq:}
   \end{split}
  \end{equation*}
The goal is to simultaneously solve the set of equations.

\subsubsection{\label{ssec:modal solution}Modal Solution}  %  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS  SSS
The first step is to compose the system
  \begin{equation}   %  =   =   =   =   =
   \begin{array}{cccc}
     \A{} & \alpha & = & y \\
     \mat{cc}{ 1 & x_{1} \\ \vdots & \vdots \\ 1 & x_{m} } &
     \mat{c}{ \alpha_{0} \\ \alpha_{1} } &
       = &
     \mat{c}{ y_{1} \\ \vdots \\ y_{m} } ,
    \label{eq:modalls}
   \end{array}
  \end{equation}
which can be expressed using the column vectors
  \begin{equation*}   %  =   =   =   =   =
   \begin{split}
     {\bf{1}} = \mat{cc}{ 1 \\ \vdots \\ 1 }, \quad
     x = \mat{cc}{ x_{1} \\ \vdots \\ x_{m} }, \quad
     y = \mat{cc}{ y_{1} \\ \vdots \\ y_{m} } .
    %\label{eq:}
   \end{split}
  \end{equation*}
The columns of the system matrix $\A{} = \mat{c|c}{\bf{1} & x}$. The solution parameters can be expressed in terms of the column vectors:
  % = =  e q u a t i o n
  \begin{equation*}
    \mat{c}{\alpha_{0} \\ \alpha_{1}} =
    \paren{ \paren{\oto}\paren{\xtx} - \paren{\otx}^{2}}^{-1}
     \bl{\mat{rr}{\xtx & -\otx \\
            -\otx &  \oto }
    \mat{c}{ \mathbf{1}^\mathrm{T}y \\  x^\mathrm{T}y}} .
  \end{equation*}
  % = =

\subsection{Errors}  %   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS   SS
When measurements are not exact, solutions are not exact. A great beauty of the method of least squares in that the quality of the solution can be quantified. An ability to discern answers like 3.0 $\pm$ 1.0 from 3.000 $\pm$ 0.0010 is invaluable. The machinery needed to compute uncertainties will be developed in following chapters.

\section{\label{sec:lsp}Least Squares Problem}  %    S    S    S    S    S    S    S    S    S    S    S    S    S    S
Emboldened by solutions to two basic problems, we turn attention towards formalities. Starting with a the linear system $\axeb$ where the matrix $\aicmn$, the data vector $b\in\cmplxm$, the least squares solution\index{least squares!solution!definition} $\xls$ is defined as the set
  \begin{equation}   %  =   =   =   =   =
    \xlsdef .
    \label{eq:xlsdef}
  \end{equation}
The least squares solution may be a point or it may be a hyperplane. The general solution is a combination of a particular solution (in blue) and a homogenous solution (in red):
  \begin{equation*}   %  =   =   =   =   =
   \begin{array}{cccccl}
     \xls 
       & = & \bsolnlsa{b}{y}, & \qquad y \in \cmplx{n} \\
       & = & \xbotha
    %\label{eq:}
   \end{array}
  \end{equation*}
where the matrix $\Ap$ is the pseudoinverse.


\endinput