\section{Full rank factorization}
The full rank factorization uses Gaussian reduction to find a spanning set of vectors for both $\brnga{}$ and $\brnga{*}$. As such, there is no alignment between between these spaces and no information about the scaling factors. It may come as a surprise then that we can assemble the pseudoinverse from this decomposition.

%%%
\begin{enumerate}
%
  \item Restrictions on target matrix: none.
%
  \item Computational burden: highest.
%
  \item Existence: always.
%
  \item Uniqueness: up to phase factors.
%
\end{enumerate}
%%%

Given $\aicmnr$, then a full rank factorization \cite[p. 633]{Meyer2000} of the matrix $\A{}$ is given by
\begin{equation}
  \A{} = \BC
  \label{eq:pantheon:full rank}
\end{equation}
where 
%\begin{equation}
%  \begin{split}
%    \A{} &\in \cmplxmnr, \\
%    \B{} &\in \cmplxmr_{\rho}, \\
%    \C{} &\in \cmplxrn_{\rho}. \\
%  \end{split}
%\end{equation}
%
\begin{equation}
  \A{} \in \cmplxmnr, \quad \B{} \in \cmplxmr_{\rho}, \quad \C{} \in \cmplxrn_{\rho}. \\
\end{equation}

For full rank matrices. Meyer 4.5.20, p. 221

%%
\subsection{Theory}
For $\A{}\in\cmplx{\bymn}_{\rho}$ there is a full rank factorization of the form
\begin{equation}
  \A{}=\B{}\,\C{}
\end{equation}
\begin{quote}
The foundation is the reduction of the target matrix $\A{}$ to row echelon form.
\begin{equation}
  \EAR{}{} 
\end{equation}
The column vectors of the factor $\B{}$ are the basic columns of the target matrix $\A{}$. The row vectors of the factor $\C{}$ are the nonzero rows of $\EA{}$.
\begin{enumerate}
\item $\B{\by{m}{\rho}} = \mat{c|c|c}{a_{j_{1}}&\dots&a_{j_{\rho}}}$: the $\rho$ basic columns of the target matrix;
\item $\C{\by{\rho}{n}} = \mat{c}{e_{\brac{k_{1}}}\\\vdots\\e_{\brac{k_{\rho}}}}$: the $\rho$ nonzero rows of reduced form $\EA{}$.
\end{enumerate}
\end{quote}
%%
\subsection{Sample full rank factorization}
Meyer 2.5.1, p. 67 \\
The target matrix is given by this:
\begin{equation}
  \A{} = \mat{cccc}{1&2&2&3 \\ 2&4&1&3 \\ 3&6&1&4}.
  \label{eq:pantheon:sample}
\end{equation}
Attack the problem by clearing one pivot at each step. You can verify that one reduction sequence is shown here:
\begin{equation}
  \begin{split}
    \R{} &= \G{3}\,\G{2}\,\G{1} \\
    \recip{3}
    \mat{rrr}{ 
     -1 &  2 & \phantom{-}0 \\
      2 & -1 & 0 \\
      1 & -5 & 3 } &=
    \mat{rrr}{
      1 & \frac{2}{3} & 0 \\
      \phantom{-}0 & -\frac{1}{3} & \phantom{-}0 \\
      0 & 0 & 1 }
    \mat{rrr}{
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      \phantom{-}0 & -\frac{5}{3} & \phantom{-}1 }
    \mat{rrr}{
      1 & 0 & 0 \\
     -2 & 1 & 0 \\
     -3 & \phantom{-}0 & \phantom{-}1 }
  \end{split}
\end{equation}
%
\begin{equation}
  \begin{split}
    \EARa{}{3} \\
    \recip{3}
    \mat{rrc}{ 
     -1 &  2 & 0 \\
      2 & -1 & 0 \\
      1 & -5 & 3 }
    \mat{cccc|ccc}{1&2&2&3 & 1 & 0 & 0 \\ 2&4&1&3 & 0 & 1 & 0 \\ 3&6&1&4 & 0 & 0 & 1}
      &=
    \mat{cccc|rrc}{
      \boxed{\bone}  & \btwo  & \bzero & \bone  & -1 &  2 &   0 \\ 
\arrayrulecolor{medgray}
      \bzero & \bzero & \boxed{\bone}  & \bone  &  2 & -1 & 0 \\\hline
\arrayrulecolor{black}
             0 &    0 &      0 &      0 &  1 & -5 & 3}
  \end{split}
\end{equation}
%
The reduced row echelon form reveals that the target matrix $\A{}$ has rank $\rho = 2$. The list of nonzero rows in $\EA{}$ is $k=\lst{1,2}$. Also the pivots reveal that the first and third columns are basic and the list of basic columns is $j=\lst{1,3}$.

The pivots in the matrix $\EA{}$ are in columns one and three. Therefore the basic columns of the target matrix are one and three. This specifies the column composition of the $\B{}$ matrix. The nonzero rows of the matrix $\EA{}$ are used to construct the rows of the matrix $\C{}$:
\begin{equation}
  \B{}  = {\bl{ \mat{cc}{1&2 \\ 2&1 \\ 3&1} }} \in \brnga{}, \qquad 
  \C{*} = {\bl{ \mat{cc}{1&0 \\ 2&0 \\ 0&1 \\ 1&1} }} \in \brnga{*}.
\end{equation}
Looking at $\R{}$ we see the vectors in $\brnga{}$ which map to the column vectors of $\C{*}$:
\begin{equation}
  \A{*} {\bl{ \mat{r}{-1 \\ 2 \\ 0 } }} = {\bl{ \mat{c}{1\\2\\0\\1} }}, \qquad
  \A{*} {\bl{ \mat{r}{ 2 \\-1 \\ 0 } }} = {\bl{ \mat{c}{0\\0\\1\\1} }}.
\end{equation}
%\begin{equation}
%  \begin{split}
%      \A{*} {\bl{ \mat{r}{-1 \\ 2 \\ 0 } }} &= {\bl{ \mat{c}{1\\2\\0\\1} }}, \quad
%      \A{*} {\bl{ \mat{r}{ 2 \\-1 \\ 0 } }}  = {\bl{ \mat{c}{0\\0\\1\\1} }}. \\
%      \brnga{*} &\to \brnga{} \qquad \quad \ \brnga{*} \to \brnga{}\\
%  \end{split}
%\end{equation}

 
Verify the decomposition by checking that the matrix product produces the target matrix:
\begin{equation}
  \begin{split}
    \A{} &= \B{}\,\C{},\\
    \mat{cccc}{1&2&2&3 \\ 2&4&1&3 \\ 3&6&1&4}
         &= {\bl{ \mat{cc}{1&2 \\ 2&1 \\ 3&1} }}
            {\bl{ \mat{cccc}{1&2&0&1 \\ 0&0&1&1} }}.
  \end{split}
\end{equation}

The decomposition is unique as the reduced form is unique and the columns of the target matrix are unique.

%%
\subsection{Comparison to the SVD}
For comparison, the SVD for the matrix is shown here:
\begin{equation}
  \begin{split}
    \U{}   & =
\mat{rrr}{
 {\bl{  \sqrt{\frac{17}{35}-\frac{31}{7 \sqrt{165}}} }} & 
 {\bl{ -\sqrt{\frac{17}{35}+\frac{31}{7 \sqrt{165}}} }} & 
 {\rd{  \frac{1}{\sqrt{35}} }} \\
  %
  {\bl{ \sqrt{\frac{1}{7}+\frac{5 \sqrt{\frac{5}{33}}}{14}} }} & 
  {\bl{ \sqrt{\frac{1}{7}-\frac{5 \sqrt{\frac{5}{33}}}{14}} }} & 
  {\rd{ \frac{-5}{\sqrt{35}} }} \\
  %
  {\bl{ \sqrt{\frac{13}{35}+\frac{37}{14 \sqrt{165}}} }} & 
  {\bl{ \sqrt{\frac{13}{35}-\frac{37}{14 \sqrt{165}}} }} & 
  {\rd{ \frac{3}{\sqrt{35}} }}
} \\
%
    \sig{} & =
\mat{cc|cc}{
 \sqrt{55+4 \sqrt{165}} & 0 & 0 & 0 \\
 0 & \sqrt{55-4 \sqrt{165}} & 0 & 0 \\\hline
 0 & 0 & 0 & 0
} \\
%
    \V{}   & =
\mat{rrcc}{
 {\bl{ \sqrt{\frac{1}{330} \paren{30+\sqrt{165}}} }} & 
 {\bl{ \sqrt{\frac{1}{11} -\frac{1}{2 \sqrt{165}}} }} & 
 {\rd{ \rsmthree }} & 
 {\rd{ \frac{-4}{\sqrt{33}} }} \\
%
 {\bl{ \sqrt{\frac{4}{11}+\frac{2}{\sqrt{165}}} }} & 
 {\bl{ \sqrt{\frac{4}{11}-\frac{2}{\sqrt{165}}} }} & 
  \rzero & 
 {\rd{ \frac{3}{\sqrt{33}} }} \\
%
 {\bl{  \sqrt{\frac{3}{11}-\sqrt{\frac{3}{55}}} }} & 
 {\bl{ -\sqrt{\frac{3}{11}+\sqrt{\frac{3}{55}}} }} & 
 {\rd{  \rsmthree }} & 
 {\rd{  \frac{2}{\sqrt{33}} }} \\
%
 {\bl{  \sqrt{\frac{1}{330} \paren{90+\sqrt{165}} } }} & 
 {\bl{ -\sqrt{\frac{3}{11}-\frac{1}{2 \sqrt{165}}} }} & 
 {\rd{  \rsthree }} & 
 {\rd{  \frac{-2}{\sqrt{33}} }}
} 
  \end{split}
\end{equation}

%%
\subsection{The least squares problem}
The full rank factorization can be used to assemble a pseudoinverse and therefore used to solve least squares problems. First we present and verify the pseudoinverse and then demonstrate the least squares solution.

%%%%%%%%%%%%%%
%%%%%%%%%%%%%%
\subsubsection{The pseudoinverse from the full rank factorization}
Using the full rank factorization in equation \eqref{eq:pantheon:full rank} we can assemble the pseudoinverse matrix using this prescription:
\begin{equation}
  \Ap = \C{*} \paren{\B{*}\A{}\,\C{*}}^{-1} \B{*} .
\end{equation}
We can verify this form of the pseudoinverse by confirming the Penrose criteria starting with equation \eqref{eq:penrose:a}. One problem is that because the target matrix does not, in general, have an inverse we can't manipulate the terms incise the parentheses. To resolve this term expand the pseudoinverse in terms of the decomposition matrices:
\begin{equation}
  \paren{\B{*}\A{}\,\C{*}}^{-1} = \paren{\B{*}\BC\C{*}}^{-1}.
\end{equation}
The product matrices $\BSB{*}$ and $\CCS{*}$ both have dimension $\byy{\rho}$:
\begin{equation}
  \begin{split}
    \text{rank} \paren{\BSB{*}} = \text{rank} \paren{\B{}} &= \rho , \\
    \text{rank} \paren{\CCS{*}} = \text{rank} \paren{\C{}} &= \rho .
  \end{split}
\end{equation}
We see these matrices are square and they have full rank; therefore the inverse matrix exists for both. The compound expression
\begin{equation}
  \paren{\B{*}\BC\C{*}}^{-1} = \paren{\CCS{*}}^{-1} \paren{\BSB{*}}^{-1}.
\end{equation}
The Penrose identity becomes
\begin{equation}
  \begin{split}
    \A{}\wx{*} 
      &= \Bigl( \paren{\BC}  \Bigr) \paren{\C{*} \paren{\B{*}\A{}\,\C{*}}^{-1} \B{*}} \Bigl( \BC \Bigr) \\
      &= \Bigl( \BC \Bigr) \paren{\C{*} \paren{\CCS{*}}^{-1} \paren{\BSB{*}}^{-1}  \B{*}} \Bigl( \BC \Bigr) \\
      &= \BC \\
      &= \A{} \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \checkmark
  \end{split}
\end{equation}
The first Penrose symmetry condition in equation \eqref{eq:penrose:b},
\begin{equation*}
  \paren{\AAp}^{*} = \AAp
\end{equation*}
involves showing that the product matrix is Hermitian symmetric. In terms of the full-rank factorization we see
\begin{equation}
  \begin{split}
    \AAp 
      &= \Bigl(\BC \Bigr) \paren{\C{*} \paren{\B{*}\A{}\,\C{*}}^{-1} \B{*}} \\ 
      &= \B{} \paren{\CCS{*}} \paren{\CCS{*}}^{-1} \paren{\BSB{*}}^{-1} \B{*} \\
      &= \B{} \paren{\BSB{*}}^{-1} \B{*}. 
  \end{split}
\end{equation}
We need to establish that this quantity is symmetric. Certainly we have
\begin{equation}
  \paren{\B{} \paren{\BSB{*}}^{-1} \B{*}}^{*} = \B{} \paren{\BSB{*}}^{-*} \B{*}
\end{equation}
where the quantity $\paren{\BSB{*}}^{-*}$ denotes first taking the inverse of $\BSB{*}$ and then taking the Hermitian conjugate. Because the inverse matrix exists, we can interchange the inverse operation with forming the Hermitian conjugate. That is,
\begin{equation}
  \begin{split}
    \paren{\BSB{*}}^{-*} 
      &= \paren{\paren{\BSB{*}}^{-1}}^{*} \\
      &= \biggl(\paren{\BSB{*}}^{*}\biggr)^{-1}
       = \biggl(\BSB{*}\biggr)^{-1}.
  \end{split}
\end{equation}
Therefore the product matrix $\paren{\BSB{*}}^{-1}$ is Hermitian conjugate. Therefore the product matrix $\AAp$ is Hermitian conjugate. Therefore the first Penrose symmetry condition is verified. The verification of the second Penrose symmetry condition is left as an exercise.

%%%%%%%%%%%%%%
%%%%%%%%%%%%%%
\subsubsection{Pseudoinverse example}
To compute the pseuodinverse for the matrix in equation \eqref{eq:pantheon:sample} first compute the product matrix
\begin{equation}
  \begin{split}
    \B{*} \A{}\, \C{*} =
    \mat{cc}{91 & 28 \\ 48 & 19}.
  \end{split}
\end{equation}
The pseudoinverse matrix is then
\begin{equation}
  \begin{split}
    \Ap &= \C{*} \paren{\B{*}\A{}\,\C{*}}^{-1} \B{*} \\
    \mat{rrr}{
    -37 & 10 &  29 \\
    -74 & 20 &  58 \\
    134 & -5 & -53 \\
     97 &  5 & -24 }
       &=
{\bl{ \mat{cc}{1&0 \\ 2&0 \\ 0&1 \\ 1&1} }}
      \recip{385}
      \mat{rr}{ 19 & -28 \\ -48 & 91 }
{\bl{ \mat{ccc}{1&2&3 \\ 2&1&1} }}.
  \end{split}
\end{equation}

%%%%%%%%%%%%%%
%%%%%%%%%%%%%%
\subsubsection{Full rank factorization and least squares}
We return to the fiducial least squares problem presented in equation \eqref{eq:???}
\begin{equation*}
  \begin{split}
    \axaeb \\
    \matrixb \xtwo &= \datab
  \end{split}
\end{equation*}
The pseudoinverse matrix for this problem is shown in table \eqref{eq:7.2}. The solution for the least squares problem is then
\begin{equation*}
  \begin{split}
      x  &= \Ap  b \\
   \xtwo &= \Aplusb \datab = \xlsb.
  \end{split}
\end{equation*}



\endinput