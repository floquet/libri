\section{Matrices}
For clarity matrices are typeset in emboldened letters. Throughout this book we will be considering matrices in general as a collection of complex numbers with $m$ rows and $n$ columns. The height of the matrix is determined by $m$, the number of rows; the width by $n$ the number of columns. The important point is that the number of rows are specified first, then the number of columns:
\begin{equation}
  \paren{\by{rows}{columns}}=\paren{\by{m}{n}}.
\end{equation}
The first row is the top row and the first column is the left-most column.

Formally we specify a matrix as
\begin{equation}
  \A{}\in\cmplx{\by{m}{n}}
\end{equation}
even if only one entry is complex. If all matrix entries are real we instead write
\begin{equation}
  \A{}\in\real{\by{m}{n}}.
\end{equation}
It is not so much the collection of numbers that is of interest here as the meaning of the rows and columns. But other properties require explanation first.

%%
\subsection{Matrix rank}
A critical matrix property is rank\index{rank!matrix}, denoted here by the parameter $\rho$.  The rank details the number of independent rows or columns. You may speak of row rank\index{rank!row}, the number of independent rows, or column rank\index{rank!column}, the number of independent columns. However the matrix rank is the same as the row rank and the same as the column rank. 
$$
\text{matrix rank} = \text{row rank} = \text{column rank}.
$$
Below are some examples of $\bys{3}$ matrices of different ranks in reduced form.
\begin{equation}
\begin{array}{ccc}
\mat{rcr}
{
-2 & e^{-1} & 1\\
 0 &  1 & 0\\
 0 &  0 &-4
}, \qquad & 
\mat{ccc}
{
 1 &  2 & 0\\
 0 &  8 & \pi\\
 0 &  0 & 0
}, \qquad & 
\mat{rrc}
{
\sqrt[4]{2} & -1 & \pi^{2}\\
 0 &  0 & 0\\
 0 &  0 & 0
} \\[5pt]
\rho = 3 & \rho = 2 & \rho = 1
\end{array}
\end{equation}

Certainly then the rank can be no larger than the minimum of the dimension parameters $m$ and $n$. That is
\begin{equation}
  \rho \le \min \lst{m,n}.
\end{equation}
To include the rank in the matrix specification use a subscript as shown here
\begin{equation}
  \A{}\in\cmplx{\by{m}{n}}_{\rho}.
\end{equation}

For example, consider a matrix with three rows, but only two are linearly independent. The second row is equal to two times the first row minus the third row. This matrix is
\begin{equation}
  \A{}\in\cmplx{\by{3}{columns}}_{2} = \mat{c}
  {
  r_{1}\\ \hline
  2r_{1}-r_{3}\\ \hline
  r_{3}
  }.
\end{equation}
Because there are two linearly independent rows, row 1 and row 3, the row rank of this matrix is $\rho = 2$. Therefore the matrix rank is also two. Notice that the number of columns must be $n \ge \rho$. 

The same principle applies to the columns. Here is a matrix where the first column is the sum of last three columns. This matrix is
\begin{equation}
  \A{}\in\cmplx{\by{rows}{4}}_{3} = \mat{c|c|c|c}
  {
  c_{2}+c_{3}-c_{4} & c_{2} & c_{3} & c_{4}
  }.
\end{equation}
Because there are three linearly independent columns, rows 2, 3, and 4, the column rank of this matrix is $\rho = 3$. Therefore the matrix rank is also three. The number of rows must be $m \ge \rho$.

The matrices generated by the outer product are all rank one matrices as shown by equation \eqref{prelim:vectors:outer}. Each row is a multiple of the input row vector.

The concept of matrix rank is a rich one which will prove invaluable. Further explanation is deferred.

%%
\subsection{Matrix multiplication}
There are a few ways to address matrix multiplication\index{matrix!multiplication}. The paradigm of interest here is to cast matrix multiplication as a series of dot products between row vectors and column vectors. For the matrix equation
\begin{equation}
  \begin{array}{cccc}
  \A{}&\B{} &=& \C{}\\
  \paren{\by{m}{p}} & \paren{\by{p}{n}} && \paren{\by{m}{n}}
  \end{array}
  \label{eq:mprod}
\end{equation}
the element of the product matrix $\C{}$ in row $r$ and column $c$ is given by the dot product of the $r$th row of $\A{}$ with the $c$th column of $\B{}$. The result is the scalar $\C{}_{r,c}$. This equation may simpler to understand than the words:
\begin{equation}
  \C{}_{r,c} = \underbrace{\A{}_{r,*}}_{\text{row }r}\cdot\underbrace{\B{}_{*,c}}_{\text{col }c}.
  \label{eq:dot}
\end{equation}
An schematic example is shown in figure \eqref{fig:1:mmult} where $\C{}_{2,4} = \A{}_{2,*}\cdot\B{}_{*,4}.$
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[]{pdf/prelim/multiply.pdf} 
   \caption{Each cell in a product matrix is computed as a dot product. Row vectors on the left matrix are dotted with column vectors in the right matrix. The concept of conformability for $\A{}\B{}=\C{}$ is distinctly shown here as both vectors must have the same length.}
   \label{fig:1:mmult}
\end{figure}
The dot product rule dictates the conformability condition\index{conformability condition}. For a product matrix of dimension $\by{m}{n}$ the component matrices must have the dimensions
of $\by{m}{p}$ for the left matrix and $\by{p}{n}$ for the right matrix as shown in equation \eqref{eq:mprod} and emphasized here
\begin{equation}
\paren{\by{m}{p}}\paren{\by{p}{n}}=\by{m}{n}.
\end{equation}
The crucial observation is that the vectors in the dot product have common dimension $p$.

Because it is easy to confuse the convention\footnote{Meyer has a very nice explanation of the source of the convention in his book, \cite[p. 123]{Meyer}.} an example follows. Think of an example of two matrices multiplied in different orders. Take a wide matrix that is $m \times n = 2\times5$ and a tall matrix that is $5\times 2$. Multiply wide by tall, then tall by wide and look at the shape of the product matrices. This realization will be recalled in the section on product matrices,
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
\includegraphics[ width = 3.75in ]{pdf/prelim/mult_prods}\\
   \caption{Shape dynamics. The product matrix inherits the height from the first multiplicand, the width from the second. That is height from the left, width from the right.}
   \label{fig:example}
\end{figure}

%%
\subsection{The matrix transpose}
The matrix transpose appears over and over throughout linear algebra and is a generalization of vector transposition.

Taking the transpose of a matrix means interchanging the rows and columns. Row 1 becomes column 1, row 2 becomes column 2, and so on. The rule has the result
\begin{equation}
  \begin{split}
    \A{}&\in\cmplx{\by{m}{n}},\\
    \A{T}&\in\cmplx{\by{n}{m}},
  \end{split}
\end{equation}
showing the interchange of $m$ and $n$.

If $\A{}$ had $m=5$ rows and $n=2$ columns the transpose $\A{T}$ will have $m=2$ rows and $n=5$ columns. 
Taking the transpose of the transpose restores the target matrix to original form. That is,
\begin{equation}
  \paren{\A{T}}^{\mathrm{T}}=\A{}.
\end{equation}

An elementary exercise, e.q. \cite[p. 10]{Meyer}, \cite[p. 8]{Strang}, shows the reverse order rule for matrix products:
\begin{equation}
  \paren{\A{}\B{}}^{\mathrm{T}}=\B{T}\A{T}.
  \label{eq:mattran}
\end{equation}

Philosophically, neither matrix nor transpose is a more fundamental object than the other. Certainly in terms of a specific computation there will be a preference flowing from the association with the rows and columns. But without an association to a measurement system, a matrix and its transpose share equivalence. Ultimately this lack of ascendancy will lead to the concept of domain and codomain.

%%
\subsection{Matrices do not commute}
Matrices share many properties with scalars like associativity and distributivity. But matrices do not commute in general. That is typically
\begin{equation}
  \brac{\A{},\B{}} = \A{}\B{}-\B{}\A{} \ne \zero.
  \label{eq:matcom}
\end{equation}

A quick way to realize this is to imagine a rectangular matrix where $m\ne n$. The product matrix $\A{}\A{T}$ is a square matrix of dimensions $\by{m}{m}$ and the product matrix $\A{T}\A{}$ is a square matrix of dimensions $\by{n}{n}$. We cannot difference these matrices of different sizes.

What if the problem is restricted to the class of square matrices where $m=n$? Then the issue is that we are using \textit{different vectors} in the dot products. Rewrite \eqref{eq:matcom} as 
\begin{equation}
  \brac{\A{},\B{}} = \A{}\B{}-\B{}\A{} = \overrightarrow{\C{}} - \overleftarrow{\C{}}
\end{equation}
and consider the matrix elements 
\begin{equation}
  \begin{split}
    \overrightarrow{\C{}}_{r,c} &= \A{}_{r,*}\cdot\B{}_{*,c},\\
    \overleftarrow{\C{}}_{r,c} &= \B{}_{r,*}\cdot\A{}_{*,c}.\\
  \end{split}
\end{equation}
In general
\begin{equation}
   \overrightarrow{\C{}}_{r,c} \ne \overleftarrow{\C{}}_{r,c}
\end{equation}
because
\begin{equation}
  \A{}_{r,*}\cdot\B{}_{*,c} \ne \B{}_{r,*}\cdot\A{}_{*,c}.
\end{equation}
The issue is that in one direction, $\A{}\B{}$, we are using the \textit{row} vectors of $\A{}$. The other direction, $\B{}\A{}$, involves the \textit{column} vectors of $\A{}$. Clearly, the general matrix does not enforce an equality between row and column vectors.

%%
\subsection{The adjoint}
One of the most important operations we will perform on a matrix is the \textit{adjoint}\index{adjoint} operation, also called \index{Hermitian conjugation}\textit{Hermitian conjugation.} This interchanges the rows and columns and requires that we take the complex conjugate of each matrix entry. If $a_{r,c}$ is the element in row $r$ and column $c$ the matrix $\A{}$ then we can symbolically represent this process as 
\begin{equation}
  \begin{split}
    \A{} &\to \A{*}\\
    a_{r,c} &\to \overline{a}_{c,r}
  \end{split}
\end{equation}

The complex conjugate of this element will appear in row $c$ and column $r$ in the matrix $\A{*}$.

When working with complex matrices
\begin{equation}
    \A{*}= \overline{\A{}}^{\mathrm{T}}= \overline{\A{T}}.
\end{equation}
The operations of conjugation and transposition commute; that is, they can be performed in any order as shown above.

Quite often we will restrict our attention to the field of real numbers and dispense with the generality of the complex field. In these cases the adjoint is the transpose.

%%
\subsection{Product matrices}
Product matrices are an important topic for this book. In general the matrices that we study are not square which makes it impossible to discuss important properties like eigenvalues. To extend these concepts to rectangular matrices, form either of the square matrices
\begin{equation}
  \begin{split}
    \W{x} &= \prdm{*}, \\
    \W{y} &= \prdmm{*}.
    \label{eq:prelim:w}
  \end{split}
\end{equation}
The astute reader may wonder about the assignment of the subscripts for the product matrices. The convention is rudimentary:
\begin{enumerate}
\item $\W{x}\in\cmplx{\by{n}{n}}$ and is conformable with $n-$vectors from the \textit{domain};
\item $\W{y}\in\cmplx{\by{m}{m}}$ and is conformable with $m-$vectors from the \textit{codomain}.
\end{enumerate}
This foreshadows the richer context of matrix domain.

While the term ``product matrix'' has the general connotation as in equation \eqref{eq:mprod}, in this work ``product matrices'' implies $\W{x}$ and $\W{y}$, the two situations where a matrix is multiplied with its adjoint.

%%
\subsection{Left and right operations}
A recurring theme in this book are left and right operations on matrices. 
The usual condition is when the target matrix premultiplies a vector as in 
\begin{equation}
\A{}x=y.
\label{eq:axy}
\end{equation}
Another option is postmultiplication of the vector by the matrix. This can be expressed by premultiplication of the transpose matrix upon the vector transpose:
\begin{equation}
  \paren{y^{\mathrm{T}}\A{}}^{\mathrm{T}} = \A{T}y=x.
\label{eq:ayx}
\end{equation}
These last two statements have deep implications. We see that the target matrix operates on an $n-$vector $x$ and returns an $m-$vector $y$. Also, the transpose matrix operates on an $m-$vector $y$ and returns an $n-$vector $x$. (Please be very clear that the vector pairs $(x,y)$ in \eqref{eq:axy} are different from the vector pairs $(x,y)$ in \eqref{eq:ayx}. The transpose is not an inverse; the transpose always exists, the inverse sometimes so.)

We saw that matrices operate upon column vectors on the right and that row vectors on the left operate upon matrices.

When we examine the pseudoinverse, the generalized matrix inverse, we will that every matrix $\A{}$ has a pseudoinverse $\A{+}$. In some cases this will be a left inverse
\begin{equation}
  \leftinv = \I{n},
\end{equation}
in other cases it may be a right inverse
\begin{equation}
  \rightinv = \I{m}.
\end{equation}
In the first case premultiplication by the pseudoinverse produced an identity matrix. In the second case postmultiplication produced an identity matrix.

%%
\subsection{Domain and codomain}
A matrix is a map between two vector spaces, the space of $m-$vectors and the space of $n-$vectors. So both spaces are domains. By convention the space of $n-$vectors is called the \index{domain and codomain}\textit{domain} and the space of $m-$vectors is called the \index{codomain}\textit{codomain}. Therefore neither domain nor codomain is a more fundamental object.

Going back we can read this equation
\begin{equation*}
  \A{}x=y
\end{equation*}
as ``The matrix $\A{}$ maps $n-$vectors in the domain $X$ to $m-$vectors in the codomain $Y$.''
We can read the equation
\begin{equation*}
  \A{*}y=x
\end{equation*}
as ``The adjoint matrix $\A{*}$ maps $m-$vectors in the codomain $Y$ to $n-$vectors in the domain $X$.''

More formally, the vector space $X$ is the set of all vectors of length $n$. Of course then the vector space $Y$ is the set of all vectors of length $m$.

To close this section we present a summary table.
\begin{equation}
\boxed{
\begin{array}{llcl}
  \text{matrix} & \text{maps from} & & \text{maps to}\\\hline\hline
  \A{} & \text{domain} &\to& \text{codomain}\\
  & n-\text{vectors} &\to& m-\text{vectors}\\
  & X &\to& Y\\ \hline
  \A{*}& \text{codomain} &\to& \text{domain}\\
  & m-\text{vectors} &\to& n-\text{vectors}\\
  & Y &\to& X
\end{array}
}
\end{equation}

%%
\subsection{The range of a matrix}
A bedrock concept of linear algebra is the \textit{range}\index{range} of a matrix. The range of a matrix $\A{}$ is the collection of all possible $m-$vectors generated by the matrix $\A{}$ when acting upon $n-$vectors:
\begin{equation}
  \rng{\A{}}=\lst{\A{}x\colon x \in \cmplx{n}}\subseteq \cmplx{m}.
\end{equation}

For example the range of the identity matrix
\begin{equation}
  \I{2}=\mat{cc}{1&0\\0&1}
\end{equation}
is the entire plane $\real{2}$. Any point in the plane can be reached with the appropriate $x$ vector. The arbitrary point 
\begin{equation}
  p = \mat{c}{\alpha \\ \beta}
\end{equation}
can be reached by using the vector
\begin{equation}
  x = \mat{c}{\alpha \\ \beta}
\end{equation}
since 
\begin{equation}
  \A{} x = p.
\end{equation}

However for matrices that do not have full rank there will always be restrictions on the range. It will not be $\real{m}$. The matrix
\begin{equation}
  \A{}=\mat{cc}{1&0\\0&0}
\end{equation}
can only produce vectors on the $x-$axis. There are no vectors which map to the any point with a non-zero $y-$coordinate. For example, there exists no such vector $x$ which solves 
\begin{equation}
  \A{}x=\mat{cc}{1&0\\0&0}\mat{c}{x_{1}\\x_{2}} = \mat{c}{0\\1}.
\end{equation}

The geometric interpretation of range\index{range!geometric interpretation} leads to the a different perspective of matrix actions upon vectors. When a matrix acts upon a vector the result is a linear combination of the column vectors of the matrix as shown below:
\begin{equation}
\begin{split}
  \A{}x&=\mat{c|c|c|c}{\A{}_{*,1} & \A{}_{*,2} & \A{}_{*,3} & \hdots}\mat{c}{x_{1} \\ x_{2} \\ x_{3} \\ \vdots} \\
    &= x_{1} \A{}_{*,1} + x_{2} \A{}_{*,2} + x_{3} \A{}_{*,3} + \dots .
\end{split}
\end{equation}
In this context, the range of a matrix is all possible linear combinations of the column vectors. Since this concept can also be framed as all possible combinations of the column vectors, the range is sometimes called the image space\index{image space} (or image) of a matrix.

%%
\subsection{The condition number}
The condition number, $\kappa$, is one of the most important diagnostic quantities we have for a matrix. It is a measure of the precision of the inverse mapping. A simple example illustrates the point. Examine the matrix sequence
\begin{equation}
\begin{array}{ccccc}
  \mat{cc}{1&0\\0&1} & \to & \mat{cc}{1&0\\0&\epsilon} & \to & \mat{cc}{1&0\\0&0}\\[15pt]
  \kappa = 1 && \kappa = 1/\epsilon && \kappa = \infty.
\end{array}
\end{equation}
The matrix on the left, the identity matrix has the ideal condition number of unity. The matrix on the right is rank-deficient and has infinite condition number. The matrix in the middle has ambiguous condition number and we see that as the parameter $\epsilon$ varies from 1 to 0 the conditioning goes from ideal to disasterous. In fact, one would certainly suspect that as $\epsilon$ nears the machine epsilon of your computer computations involving this matrix will be unreliable.

A more formal definition will follow later because the emphasis here is on intuition. For now consider three basic cases:
\begin{enumerate}
\item The ideal case, $\kappa=1$, for exact maps like identity matrices. This means that all vectors $\paren{y_{1},y_{2}}^{\mathrm{T}}$ in the codomain  can be exactly mapped to all source vectors $\paren{x_{1},x_{2}}^{\mathrm{T}}$ in the domain by the matrix inverse.
\item Imprecise maps where the condition number is large. Some vectors $\paren{y_{1},y_{2}}^{\mathrm{T}}$ in the codomain may be mapped by the inverse not to the source vector $\paren{x_{1},x_{2}}^{\mathrm{T}}$, but instead to a nearby vector.
\item Frustrated maps where $\kappa=\infty$. Entire classes of vectors $\paren{y_{1},y_{2}}^{\mathrm{T}}$ in the codomain cannot be mapped to any vectors in the domain $\paren{x_{1},x_{2}}^{\mathrm{T}}$.
\end{enumerate}

%%
\subsection{A note on terminology}
Basic terminology differences distinguish between real and complex matrices. If all entries in a matrix are real, then the matrix is real. If at least one matrix entry is imaginary or complex, then the matrix is complex. Many texts avoid stirring complex numbers into the mix. The philosophy here is that the added intricacy of complex arithmetic is more than offset by the unique properties we will explore in the complex realm.

The table below shows the differences in terminology between real and complex matrices.
\begin{equation}
\boxed{
\begin{array}{cc}
  \text{real matrices} & \text{complex matrices} \\\hline\hline
  \text{transpose} & \text{adjoint} \\
  \A{T} & \A{*} \  \paren{= \overline{\A{T}}} \\[10pt]\hline
  \text{orthogonal} & \text{unitary} \\
  \A{-1}=\A{T} & \A{-1}=\A{*}
\end{array}
}
\end{equation}

The complex notation is more general and should be used if the matrix entries are not restricted to be real numbers. However if all matrix entries are real the custom is to use notation for real matrices. 

\endinput