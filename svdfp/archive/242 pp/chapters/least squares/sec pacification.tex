\section{Regularization}
In the previous section we saw that whenever $\datarange\ne0$ it is possible to regularize inconsistent linear systems and formulate a linear system such that $x=\xmp$. Formally linear systems of the form
\begin{equation*}
  \axeb
\end{equation*}
can be pacified and rendered consistent by recasting them as
\begin{equation*}
  \A{}\, \xbl = \datarange.
\end{equation*}
The solution to both equations is the same vector. The advantage is going from a least squares method to a Gaussian elimination method. 


To do so we exploit the information in the \asvd, specifically domain matrix $\U{}$. We demonstrate this technique using the example matrices (a) and (b) and sample decompositions are given in table \eqref{tab:svden:compare svds}. In these examples the least squares solutions will appear spontaneously. A later section will detail finding these solutions.

%%%%%%%%%%%
\subsection{Example matrix (a)}
The linear system of interest is
%
\begin{equation}
  \begin{split}
    \Ax &= b \\
    \matrixa \xthree &= \dataa.
  \end{split}
  \label{eq:ls:a}
\end{equation}
%
There is no exact solution for this system. To regularize this system we turn to the domain matrices which are
\begin{equation}
  \U{} = \matrixaY, \quad \V{} = \matrixaX .
\end{equation}
%
One way to write the full least squares solution\index{least squares!solutions!example (a)} is
%
\begin{equation}
  x_{LS} = \xthree = \underbrace{\xlsa}_{\brnga{*}} + \underbrace{\alpha_{1} \nllaou  + \alpha_{2} \nllatu}_{\rnlla{}}, \quad \alpha\ic.
\end{equation}
%
The \ns\ vectors are taken directly from the codomain matrix $\V{}$. The range space component of this solution matches the first column of the matrix $\V{}$.  

The pseudoinverse or point solution is form of $\xls$ which has the least norm. This occurs when $\alpha_{1} = \alpha_{2} = 0$.
%
\begin{equation}
  \xmp = \xlsa
  \label{eq:solnmp:a}
\end{equation}
%
The residual error for the least squares solution is
%
\begin{equation}
  {\rd{ r }} = \A{}\,\bxls - b = \dataar \quad \forall \alpha\ic.
\end{equation}
%
We will see that the error represents the \ns\ component of the data vector $b$. That is,
%
\begin{equation}
  \datanull = {\rd{ r }}.
\end{equation}

%
The point is to find the orthogonal resolution of the data vector. You may compute $\datarange$ directly, or you may compute $\datanull$ and subtract that from $b$. For reinforcement, we compute both range and \ns\ components directly. Start with the amplitudes of the projections along the basis vectors identified in $\V{}$. 
\begin{equation}
  \begin{split}
    c_{1} = b \cdot \buo &= \recip{\sqrt{2}} , \\
    c_{2} = b \cdot \rut &= \frac{3}{\sqrt{2}} .
  \end{split}
\end{equation}
%
The separated form of the data vector is now
%
\begin{equation}
  \begin{split}
     b &= \datarange + \datanull, \\
       &= c_{1} \buo + c_{2} \rut, \\
       &= c_{1}\obvecam + c_{2}\orvecan, \\
     \dataa &= {\bl{ \half \mat{r}{ 1 \\ -1 } }} + {\rd{ \frac{3}{2}\mat{c}{1\\1} }}.
  \end{split}
  \label{eq:regularize:a}
\end{equation}
%
The linear system in equation \eqref{eq:ls:a} is pacified by removing the \ns\ component of the data vector. The resulting consistent linear system is
\begin{equation}
  \begin{split}
    \A{}\, \xbl &= \datarange, \\
    \matrixa \xlsa &= {\bl{ \half \mat{r}{ 1 \\ -1 } }} .
  \end{split}
\end{equation}
The system surrenders an answer using Gaussian elimination,
%
\begin{equation}
  \xbl = \xlsa,
\end{equation}
and reveals the same solution as in equation \eqref{eq:solnmp:a}.

%%%%%%%%%%%
\subsection{Example matrix (b)}
Here the linear system of interest is
%
\begin{equation}
  \begin{split}
    \Ax &= b \\
    \matrixb \xtwo &= \datab.
  \end{split}
  \label{eq:ls:b}
\end{equation}
%
Again there is no exact solution for this system.
%
The domain matrices are
\begin{equation}
  \U{} = \matrixbY, \quad \V{} = \matrixbX .
\end{equation}
%
The full least squares solution\index{least squares!solutions!example (b)} is the point
%
\begin{equation}
  \bxls = {\bl{ \xtwo }} = \xlsb.
\end{equation}
%

Because the \ns\ $\rnlla{*}$ is trivial the pseudoinverse solution and the least squares solution coincide.
%
\begin{equation}
  \xmp = \bxls = \xlsb
  \label{eq:solnmp:b}
\end{equation}
%
The residual error is
%
\begin{equation}
  {\rd{ r }} = \A{}\,\bxls - b = \databr.
\end{equation}
%
Computational detail of computing the projection amplitudes follow the same pattern as in the last section.
%
\begin{equation}
  \begin{split}
     b &= \datarange + \datanull, \\
       &= \frac{7}{\sqrt{30}} \paren{\obvecbm}  + \recip{\sqrt{6}} \paren{\obvecbn}  - \frac{4}{\sqrt{5}} \paren{\orvecbo} , \\
     \datab &= {\bl{ \recip{5} \mat{c}{2\\5\\4}}} + {\rd{ \recip{5} \mat{r}{8\\0\\-4}}}
  \end{split}
  \label{eq:regularize:b}
\end{equation}
%
The linear system in equation \eqref{eq:ls:b} is pacified by removing the \ns\ component of the data vector. The resulting consistent linear system is
\begin{equation}
  \begin{split}
    \A{}\, \xbl &= \datarange, \\
    \matrixb \xlsb &= {\bl{ \recip{5} \mat{c}{2\\5\\4}}} .
  \end{split}
\end{equation}
Now we can find the least squares solution using Gaussian elimination,
%
\begin{equation}
  \xbl = \xlsb,
\end{equation}
in agreement with equation \eqref{eq:solnmp:b}.

%%%%%%%%%%%
\subsection{Example matrix (c)}
The final linear system is
\begin{equation}
  \begin{split}
    \Ax &= b \\
    \matrixc \xtwo &= \datac
  \end{split}
\end{equation}
%
This system does not require regularization. The range space is the complete space
%
\begin{equation}
  \begin{split}
    {\bl{ x }} &\in \brnga{*} \subseteq \cmplx{2}, \\
    {\bl{ b }} &\in \brnga{} \subseteq \cmplx{2}.
  \end{split}
\end{equation}
Therefore the data vector must be in the range space.

%%%%%%%%%%%
\subsection{Conclusion}
In the previous sections we saw how the Method of Least Squares is a form of regularization. We saw examples where singular problems we converted to nonsingular systems. The point is that the least squares solution is finding the best solution for the singular systems.

We exploited the SVD to regularize the problem, in essence finding the projection of the data vector into the range space. The SVD solution comes at a significant computational price. Typically, 

\endinput